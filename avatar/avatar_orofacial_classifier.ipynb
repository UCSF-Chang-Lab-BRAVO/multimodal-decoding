{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avatar decoding example\n",
    "#### Training a classifier to distinguish 6 non-speech articulatory movements in the articulatory-movement task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  2 21:24:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN V      Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 28%   31C    P8    24W / 250W |   1815MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN V      Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 28%   31C    P8    24W / 250W |   2177MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA TITAN V      Off  | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 28%   29C    P8    24W / 250W |      4MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2511299      C   ...R2019a/bin/glnxa64/MATLAB     1811MiB |\n",
      "|    1   N/A  N/A   2238041      C   ...nda3/envs/ecog/bin/python     2173MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Check if you have a GPU\n",
    "!nvidia-smi\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - change this to be your own directory. \n",
    "data_dir= 'YOUR_DATA_DIR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load up the helper functions \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--decimation', \n",
    "                   default=6, \n",
    "                   type=int, \n",
    "                   help='How much to decimate')\n",
    "parser.add_argument('--start_time', \n",
    "                    type=float,\n",
    "                   default=1,\n",
    "                   help='how much time before cue')\n",
    "parser.add_argument('--end_time',\n",
    "                    type=float,\n",
    "                   default=3, \n",
    "                   help= 'how much time after the cue')\n",
    "parser.add_argument('--hidden_dim',\n",
    "                    type=int,\n",
    "                   default=512,\n",
    "                   help=\"how many hid.units\")\n",
    "parser.add_argument('--lr', \n",
    "                    type=float,\n",
    "                   default=1e-3,\n",
    "                   help='learning rate')\n",
    "parser.add_argument('--ks', \n",
    "                    type=int,\n",
    "                   default=1,\n",
    "                   help='ks at input.')\n",
    "parser.add_argument('--num_layers',\n",
    "                   type=int, \n",
    "                   default=2,\n",
    "                   help='number of layers')\n",
    "parser.add_argument('--dropout', \n",
    "                   type=float, \n",
    "                   default=0.6, \n",
    "                   help='dropout amount')\n",
    "parser.add_argument('--feat_stream', \n",
    "                   type=str, \n",
    "                   default='both',\n",
    "                   help='which stream. both, hga, or raw')\n",
    "parser.add_argument('--bs',\n",
    "                   type=int, \n",
    "                   default=32, \n",
    "                   help='batch size')\n",
    "parser.add_argument('--smooth',\n",
    "                   type=int,\n",
    "                   default=10)\n",
    "parser.add_argument('--no_normalize', \n",
    "                   action='store_false',\n",
    "                   help='no keras normalisation')\n",
    "parser.add_argument('--maxpool_ks', type=int, default=10,\n",
    "                    help='how much to go')\n",
    "args = vars(parser.parse_args( '--num_layers 1 --no_normalize'.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "X = np.load(join(data_dir, 'X_avatar_pc.npy'))\n",
    "Y = np.load(join(data_dir, 'Y_avatar_pc.npy'))\n",
    "\n",
    "# The Y's correspond to the following avatar movements: \n",
    "# 0: Lips back\n",
    "# 1: Lips forward\n",
    "# 2: Mouth closed\n",
    "# 3: Mouth open\n",
    "# 4: Tongue down\n",
    "# 5: Tongue up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features from the neural data\n",
    "X_m = np.max(X[:, :62], axis=1)\n",
    "X_n = np.min(X[:, :62], axis=1)\n",
    "X_mu = np.mean(X[:, :62], axis=1)\n",
    "X_s = np.std(X[:, :62], axis=1)\n",
    "X_m2 = np.max(X[:, 62:], axis=1)\n",
    "X_n2 = np.min(X[:, 62:], axis=1)\n",
    "X_mu2 = np.mean(X[:, 62:], axis=1)\n",
    "X_s2 = np.std(X[:, 62:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 506)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.hstack((X_m, X_n, X_mu, X_s, X_m2, X_n2, X_mu2, X_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 4048)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.arange(760)\n",
    "val = np.arange(760, 800)\n",
    "test_final= np.arange(800, 840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 4048)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smetzger/.conda/envs/silent_spelling/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import MLP # import a simple model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = (0.6, 2, 512) # Hyperparameters used, \n",
    "# these were found via tuning using the final 40 samples as the hyperparameter tuning set, so we exclude those below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minct 140\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.620, te_loss: 1.381, acc: 0.369, te acc: 0.550\n",
      "chk\n",
      "epoch 1, loss: 1.171, te_loss: 1.025, acc: 0.619, te acc: 0.738\n",
      "chk\n",
      "epoch 2, loss: 0.853, te_loss: 0.766, acc: 0.747, te acc: 0.787\n",
      "chk\n",
      "epoch 3, loss: 0.640, te_loss: 0.597, acc: 0.836, te acc: 0.887\n",
      "chk\n",
      "epoch 4, loss: 0.481, te_loss: 0.494, acc: 0.889, te acc: 0.850\n",
      "epoch 5, loss: 0.357, te_loss: 0.431, acc: 0.902, te acc: 0.863\n",
      "epoch 6, loss: 0.299, te_loss: 0.355, acc: 0.919, te acc: 0.912\n",
      "chk\n",
      "epoch 7, loss: 0.229, te_loss: 0.326, acc: 0.958, te acc: 0.900\n",
      "epoch 8, loss: 0.169, te_loss: 0.313, acc: 0.967, te acc: 0.900\n",
      "epoch 9, loss: 0.147, te_loss: 0.314, acc: 0.972, te acc: 0.875\n",
      "epoch 10, loss: 0.106, te_loss: 0.265, acc: 0.980, te acc: 0.875\n",
      "epoch 11, loss: 0.085, te_loss: 0.279, acc: 0.986, te acc: 0.887\n",
      "epoch 12, loss: 0.070, te_loss: 0.263, acc: 0.989, te acc: 0.875\n",
      "epoch 13, loss: 0.055, te_loss: 0.215, acc: 0.997, te acc: 0.925\n",
      "chk\n",
      "epoch 14, loss: 0.052, te_loss: 0.208, acc: 0.995, te acc: 0.900\n",
      "epoch 15, loss: 0.038, te_loss: 0.242, acc: 0.997, te acc: 0.900\n",
      "epoch 16, loss: 0.035, te_loss: 0.218, acc: 0.997, te acc: 0.912\n",
      "epoch 17, loss: 0.025, te_loss: 0.225, acc: 0.998, te acc: 0.900\n",
      "epoch 18, loss: 0.020, te_loss: 0.195, acc: 1.000, te acc: 0.925\n",
      "epoch 19, loss: 0.020, te_loss: 0.192, acc: 1.000, te acc: 0.925\n",
      "epoch 20, loss: 0.016, te_loss: 0.210, acc: 1.000, te acc: 0.912\n",
      "epoch 21, loss: 0.016, te_loss: 0.213, acc: 1.000, te acc: 0.900\n",
      "epoch 22, loss: 0.014, te_loss: 0.216, acc: 1.000, te acc: 0.925\n",
      "epoch 23, loss: 0.018, te_loss: 0.202, acc: 0.998, te acc: 0.912\n",
      "epoch 24, loss: 0.012, te_loss: 0.210, acc: 1.000, te acc: 0.900\n",
      "epoch 25, loss: 0.015, te_loss: 0.193, acc: 0.998, te acc: 0.887\n",
      "epoch 26, loss: 0.015, te_loss: 0.212, acc: 0.998, te acc: 0.900\n",
      "epoch 27, loss: 0.012, te_loss: 0.225, acc: 1.000, te acc: 0.887\n",
      "epoch 28, loss: 0.008, te_loss: 0.191, acc: 1.000, te acc: 0.925\n",
      "epoch 29, loss: 0.007, te_loss: 0.213, acc: 1.000, te acc: 0.900\n",
      "epoch 30, loss: 0.009, te_loss: 0.203, acc: 1.000, te acc: 0.900\n",
      "epoch 31, loss: 0.009, te_loss: 0.192, acc: 1.000, te acc: 0.912\n",
      "epoch 32, loss: 0.007, te_loss: 0.203, acc: 1.000, te acc: 0.925\n",
      "epoch 33, loss: 0.006, te_loss: 0.204, acc: 1.000, te acc: 0.912\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.330, acc: 0.000, te acc: 0.850\n",
      "epoch 1, loss: 0.000, te_loss: 0.330, acc: 0.000, te acc: 0.850\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.685, te_loss: 1.461, acc: 0.322, te acc: 0.575\n",
      "chk\n",
      "epoch 1, loss: 1.255, te_loss: 1.088, acc: 0.634, te acc: 0.762\n",
      "chk\n",
      "epoch 2, loss: 0.919, te_loss: 0.824, acc: 0.753, te acc: 0.812\n",
      "chk\n",
      "epoch 3, loss: 0.670, te_loss: 0.681, acc: 0.831, te acc: 0.787\n",
      "epoch 4, loss: 0.526, te_loss: 0.596, acc: 0.866, te acc: 0.850\n",
      "chk\n",
      "epoch 5, loss: 0.406, te_loss: 0.469, acc: 0.897, te acc: 0.863\n",
      "chk\n",
      "epoch 6, loss: 0.288, te_loss: 0.421, acc: 0.945, te acc: 0.875\n",
      "chk\n",
      "epoch 7, loss: 0.242, te_loss: 0.368, acc: 0.941, te acc: 0.863\n",
      "epoch 8, loss: 0.179, te_loss: 0.401, acc: 0.959, te acc: 0.863\n",
      "epoch 9, loss: 0.135, te_loss: 0.374, acc: 0.981, te acc: 0.825\n",
      "epoch 10, loss: 0.117, te_loss: 0.357, acc: 0.988, te acc: 0.863\n",
      "epoch 11, loss: 0.097, te_loss: 0.344, acc: 0.986, te acc: 0.875\n",
      "epoch 12, loss: 0.073, te_loss: 0.320, acc: 0.994, te acc: 0.875\n",
      "epoch 13, loss: 0.060, te_loss: 0.297, acc: 0.992, te acc: 0.838\n",
      "epoch 14, loss: 0.041, te_loss: 0.302, acc: 1.000, te acc: 0.887\n",
      "chk\n",
      "epoch 15, loss: 0.039, te_loss: 0.329, acc: 0.997, te acc: 0.863\n",
      "epoch 16, loss: 0.040, te_loss: 0.327, acc: 0.997, te acc: 0.850\n",
      "epoch 17, loss: 0.035, te_loss: 0.336, acc: 0.997, te acc: 0.887\n",
      "epoch 18, loss: 0.029, te_loss: 0.341, acc: 0.997, te acc: 0.875\n",
      "epoch 19, loss: 0.023, te_loss: 0.310, acc: 0.998, te acc: 0.875\n",
      "epoch 20, loss: 0.018, te_loss: 0.337, acc: 1.000, te acc: 0.887\n",
      "epoch 21, loss: 0.016, te_loss: 0.306, acc: 1.000, te acc: 0.887\n",
      "epoch 22, loss: 0.018, te_loss: 0.317, acc: 1.000, te acc: 0.875\n",
      "epoch 23, loss: 0.013, te_loss: 0.338, acc: 1.000, te acc: 0.875\n",
      "epoch 24, loss: 0.015, te_loss: 0.295, acc: 1.000, te acc: 0.863\n",
      "epoch 25, loss: 0.012, te_loss: 0.312, acc: 1.000, te acc: 0.887\n",
      "epoch 26, loss: 0.010, te_loss: 0.327, acc: 1.000, te acc: 0.863\n",
      "epoch 27, loss: 0.011, te_loss: 0.337, acc: 1.000, te acc: 0.875\n",
      "epoch 28, loss: 0.011, te_loss: 0.336, acc: 0.998, te acc: 0.887\n",
      "epoch 29, loss: 0.010, te_loss: 0.317, acc: 1.000, te acc: 0.863\n",
      "epoch 30, loss: 0.008, te_loss: 0.347, acc: 1.000, te acc: 0.887\n",
      "epoch 31, loss: 0.009, te_loss: 0.334, acc: 1.000, te acc: 0.900\n",
      "chk\n",
      "epoch 32, loss: 0.007, te_loss: 0.316, acc: 1.000, te acc: 0.875\n",
      "epoch 33, loss: 0.008, te_loss: 0.352, acc: 1.000, te acc: 0.863\n",
      "epoch 34, loss: 0.010, te_loss: 0.352, acc: 0.998, te acc: 0.863\n",
      "epoch 35, loss: 0.008, te_loss: 0.347, acc: 1.000, te acc: 0.875\n",
      "epoch 36, loss: 0.005, te_loss: 0.350, acc: 1.000, te acc: 0.875\n",
      "epoch 37, loss: 0.006, te_loss: 0.325, acc: 1.000, te acc: 0.887\n",
      "epoch 38, loss: 0.005, te_loss: 0.343, acc: 1.000, te acc: 0.875\n",
      "epoch 39, loss: 0.005, te_loss: 0.323, acc: 1.000, te acc: 0.887\n",
      "epoch 40, loss: 0.005, te_loss: 0.356, acc: 1.000, te acc: 0.850\n",
      "epoch 41, loss: 0.004, te_loss: 0.371, acc: 1.000, te acc: 0.863\n",
      "epoch 42, loss: 0.005, te_loss: 0.339, acc: 1.000, te acc: 0.875\n",
      "epoch 43, loss: 0.005, te_loss: 0.343, acc: 1.000, te acc: 0.875\n",
      "epoch 44, loss: 0.004, te_loss: 0.341, acc: 1.000, te acc: 0.875\n",
      "epoch 45, loss: 0.005, te_loss: 0.360, acc: 1.000, te acc: 0.863\n",
      "epoch 46, loss: 0.003, te_loss: 0.353, acc: 1.000, te acc: 0.863\n",
      "epoch 47, loss: 0.003, te_loss: 0.347, acc: 1.000, te acc: 0.875\n",
      "epoch 48, loss: 0.003, te_loss: 0.370, acc: 1.000, te acc: 0.863\n",
      "epoch 49, loss: 0.004, te_loss: 0.340, acc: 1.000, te acc: 0.863\n",
      "epoch 50, loss: 0.003, te_loss: 0.348, acc: 1.000, te acc: 0.887\n",
      "epoch 51, loss: 0.003, te_loss: 0.377, acc: 1.000, te acc: 0.875\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.229, acc: 0.000, te acc: 0.912\n",
      "epoch 1, loss: 0.000, te_loss: 0.229, acc: 0.000, te acc: 0.912\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.622, te_loss: 1.380, acc: 0.375, te acc: 0.637\n",
      "chk\n",
      "epoch 1, loss: 1.178, te_loss: 1.053, acc: 0.644, te acc: 0.725\n",
      "chk\n",
      "epoch 2, loss: 0.865, te_loss: 0.865, acc: 0.772, te acc: 0.775\n",
      "chk\n",
      "epoch 3, loss: 0.637, te_loss: 0.695, acc: 0.828, te acc: 0.800\n",
      "chk\n",
      "epoch 4, loss: 0.493, te_loss: 0.618, acc: 0.881, te acc: 0.775\n",
      "epoch 5, loss: 0.383, te_loss: 0.535, acc: 0.892, te acc: 0.825\n",
      "chk\n",
      "epoch 6, loss: 0.307, te_loss: 0.477, acc: 0.931, te acc: 0.825\n",
      "epoch 7, loss: 0.225, te_loss: 0.435, acc: 0.950, te acc: 0.875\n",
      "chk\n",
      "epoch 8, loss: 0.190, te_loss: 0.415, acc: 0.956, te acc: 0.838\n",
      "epoch 9, loss: 0.136, te_loss: 0.382, acc: 0.980, te acc: 0.875\n",
      "epoch 10, loss: 0.096, te_loss: 0.345, acc: 0.994, te acc: 0.863\n",
      "epoch 11, loss: 0.078, te_loss: 0.348, acc: 0.992, te acc: 0.887\n",
      "chk\n",
      "epoch 12, loss: 0.076, te_loss: 0.336, acc: 0.986, te acc: 0.850\n",
      "epoch 13, loss: 0.057, te_loss: 0.326, acc: 0.998, te acc: 0.887\n",
      "epoch 14, loss: 0.047, te_loss: 0.313, acc: 0.998, te acc: 0.875\n",
      "epoch 15, loss: 0.040, te_loss: 0.304, acc: 0.998, te acc: 0.863\n",
      "epoch 16, loss: 0.032, te_loss: 0.293, acc: 1.000, te acc: 0.875\n",
      "epoch 17, loss: 0.029, te_loss: 0.291, acc: 0.998, te acc: 0.863\n",
      "epoch 18, loss: 0.027, te_loss: 0.307, acc: 0.998, te acc: 0.863\n",
      "epoch 19, loss: 0.020, te_loss: 0.314, acc: 1.000, te acc: 0.875\n",
      "epoch 20, loss: 0.021, te_loss: 0.305, acc: 0.998, te acc: 0.875\n",
      "epoch 21, loss: 0.019, te_loss: 0.300, acc: 1.000, te acc: 0.887\n",
      "epoch 22, loss: 0.015, te_loss: 0.299, acc: 1.000, te acc: 0.875\n",
      "epoch 23, loss: 0.016, te_loss: 0.287, acc: 1.000, te acc: 0.863\n",
      "epoch 24, loss: 0.011, te_loss: 0.293, acc: 1.000, te acc: 0.875\n",
      "epoch 25, loss: 0.010, te_loss: 0.300, acc: 1.000, te acc: 0.887\n",
      "epoch 26, loss: 0.010, te_loss: 0.306, acc: 1.000, te acc: 0.875\n",
      "epoch 27, loss: 0.009, te_loss: 0.302, acc: 1.000, te acc: 0.875\n",
      "epoch 28, loss: 0.010, te_loss: 0.314, acc: 1.000, te acc: 0.887\n",
      "epoch 29, loss: 0.009, te_loss: 0.303, acc: 1.000, te acc: 0.875\n",
      "epoch 30, loss: 0.008, te_loss: 0.301, acc: 1.000, te acc: 0.887\n",
      "epoch 31, loss: 0.008, te_loss: 0.300, acc: 1.000, te acc: 0.887\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.270, acc: 0.000, te acc: 0.900\n",
      "epoch 1, loss: 0.000, te_loss: 0.270, acc: 0.000, te acc: 0.900\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.609, te_loss: 1.333, acc: 0.403, te acc: 0.675\n",
      "chk\n",
      "epoch 1, loss: 1.148, te_loss: 0.944, acc: 0.678, te acc: 0.850\n",
      "chk\n",
      "epoch 2, loss: 0.829, te_loss: 0.723, acc: 0.762, te acc: 0.850\n",
      "epoch 3, loss: 0.622, te_loss: 0.566, acc: 0.839, te acc: 0.875\n",
      "chk\n",
      "epoch 4, loss: 0.491, te_loss: 0.436, acc: 0.867, te acc: 0.938\n",
      "chk\n",
      "epoch 5, loss: 0.351, te_loss: 0.355, acc: 0.933, te acc: 0.950\n",
      "chk\n",
      "epoch 6, loss: 0.276, te_loss: 0.312, acc: 0.938, te acc: 0.963\n",
      "chk\n",
      "epoch 7, loss: 0.212, te_loss: 0.265, acc: 0.966, te acc: 0.963\n",
      "epoch 8, loss: 0.168, te_loss: 0.245, acc: 0.967, te acc: 0.963\n",
      "epoch 9, loss: 0.125, te_loss: 0.217, acc: 0.981, te acc: 0.950\n",
      "epoch 10, loss: 0.098, te_loss: 0.191, acc: 0.989, te acc: 0.963\n",
      "epoch 11, loss: 0.089, te_loss: 0.177, acc: 0.991, te acc: 0.938\n",
      "epoch 12, loss: 0.072, te_loss: 0.167, acc: 0.991, te acc: 0.950\n",
      "epoch 13, loss: 0.056, te_loss: 0.176, acc: 0.994, te acc: 0.963\n",
      "epoch 14, loss: 0.044, te_loss: 0.157, acc: 0.998, te acc: 0.950\n",
      "epoch 15, loss: 0.040, te_loss: 0.150, acc: 0.998, te acc: 0.963\n",
      "epoch 16, loss: 0.032, te_loss: 0.163, acc: 1.000, te acc: 0.963\n",
      "epoch 17, loss: 0.027, te_loss: 0.158, acc: 1.000, te acc: 0.963\n",
      "epoch 18, loss: 0.025, te_loss: 0.160, acc: 0.998, te acc: 0.963\n",
      "epoch 19, loss: 0.022, te_loss: 0.168, acc: 0.998, te acc: 0.963\n",
      "epoch 20, loss: 0.018, te_loss: 0.143, acc: 1.000, te acc: 0.963\n",
      "epoch 21, loss: 0.016, te_loss: 0.154, acc: 1.000, te acc: 0.963\n",
      "epoch 22, loss: 0.014, te_loss: 0.138, acc: 1.000, te acc: 0.963\n",
      "epoch 23, loss: 0.015, te_loss: 0.139, acc: 0.998, te acc: 0.963\n",
      "epoch 24, loss: 0.015, te_loss: 0.159, acc: 0.998, te acc: 0.963\n",
      "epoch 25, loss: 0.013, te_loss: 0.172, acc: 1.000, te acc: 0.963\n",
      "epoch 26, loss: 0.011, te_loss: 0.135, acc: 1.000, te acc: 0.963\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.380, acc: 0.000, te acc: 0.900\n",
      "epoch 1, loss: 0.000, te_loss: 0.380, acc: 0.000, te acc: 0.900\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.636, te_loss: 1.406, acc: 0.348, te acc: 0.650\n",
      "chk\n",
      "epoch 1, loss: 1.186, te_loss: 1.057, acc: 0.653, te acc: 0.750\n",
      "chk\n",
      "epoch 2, loss: 0.870, te_loss: 0.829, acc: 0.744, te acc: 0.800\n",
      "chk\n",
      "epoch 3, loss: 0.644, te_loss: 0.685, acc: 0.852, te acc: 0.825\n",
      "chk\n",
      "epoch 4, loss: 0.491, te_loss: 0.577, acc: 0.883, te acc: 0.838\n",
      "chk\n",
      "epoch 5, loss: 0.361, te_loss: 0.510, acc: 0.934, te acc: 0.863\n",
      "chk\n",
      "epoch 6, loss: 0.297, te_loss: 0.452, acc: 0.933, te acc: 0.863\n",
      "epoch 7, loss: 0.235, te_loss: 0.383, acc: 0.944, te acc: 0.912\n",
      "chk\n",
      "epoch 8, loss: 0.155, te_loss: 0.370, acc: 0.984, te acc: 0.912\n",
      "epoch 9, loss: 0.136, te_loss: 0.358, acc: 0.977, te acc: 0.912\n",
      "epoch 10, loss: 0.103, te_loss: 0.318, acc: 0.988, te acc: 0.887\n",
      "epoch 11, loss: 0.080, te_loss: 0.315, acc: 0.994, te acc: 0.912\n",
      "epoch 12, loss: 0.067, te_loss: 0.284, acc: 0.994, te acc: 0.912\n",
      "epoch 13, loss: 0.051, te_loss: 0.290, acc: 1.000, te acc: 0.900\n",
      "epoch 14, loss: 0.040, te_loss: 0.326, acc: 0.997, te acc: 0.887\n",
      "epoch 15, loss: 0.041, te_loss: 0.311, acc: 0.998, te acc: 0.887\n",
      "epoch 16, loss: 0.035, te_loss: 0.296, acc: 0.998, te acc: 0.912\n",
      "epoch 17, loss: 0.034, te_loss: 0.286, acc: 0.998, te acc: 0.912\n",
      "epoch 18, loss: 0.024, te_loss: 0.284, acc: 1.000, te acc: 0.925\n",
      "chk\n",
      "epoch 19, loss: 0.021, te_loss: 0.287, acc: 0.998, te acc: 0.900\n",
      "epoch 20, loss: 0.019, te_loss: 0.270, acc: 1.000, te acc: 0.912\n",
      "epoch 21, loss: 0.017, te_loss: 0.289, acc: 1.000, te acc: 0.900\n",
      "epoch 22, loss: 0.017, te_loss: 0.292, acc: 0.997, te acc: 0.912\n",
      "epoch 23, loss: 0.016, te_loss: 0.297, acc: 0.998, te acc: 0.900\n",
      "epoch 24, loss: 0.016, te_loss: 0.305, acc: 1.000, te acc: 0.900\n",
      "epoch 25, loss: 0.011, te_loss: 0.295, acc: 1.000, te acc: 0.900\n",
      "epoch 26, loss: 0.011, te_loss: 0.298, acc: 1.000, te acc: 0.887\n",
      "epoch 27, loss: 0.009, te_loss: 0.291, acc: 1.000, te acc: 0.912\n",
      "epoch 28, loss: 0.009, te_loss: 0.287, acc: 1.000, te acc: 0.900\n",
      "epoch 29, loss: 0.009, te_loss: 0.283, acc: 1.000, te acc: 0.912\n",
      "epoch 30, loss: 0.007, te_loss: 0.289, acc: 1.000, te acc: 0.912\n",
      "epoch 31, loss: 0.008, te_loss: 0.281, acc: 1.000, te acc: 0.912\n",
      "epoch 32, loss: 0.006, te_loss: 0.290, acc: 1.000, te acc: 0.900\n",
      "epoch 33, loss: 0.009, te_loss: 0.266, acc: 0.998, te acc: 0.912\n",
      "epoch 34, loss: 0.007, te_loss: 0.265, acc: 1.000, te acc: 0.912\n",
      "epoch 35, loss: 0.007, te_loss: 0.282, acc: 1.000, te acc: 0.925\n",
      "epoch 36, loss: 0.006, te_loss: 0.282, acc: 1.000, te acc: 0.900\n",
      "epoch 37, loss: 0.004, te_loss: 0.284, acc: 1.000, te acc: 0.900\n",
      "epoch 38, loss: 0.006, te_loss: 0.288, acc: 1.000, te acc: 0.925\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.302, acc: 0.000, te acc: 0.838\n",
      "epoch 1, loss: 0.000, te_loss: 0.302, acc: 0.000, te acc: 0.838\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.602, te_loss: 1.332, acc: 0.392, te acc: 0.550\n",
      "chk\n",
      "epoch 1, loss: 1.128, te_loss: 0.966, acc: 0.659, te acc: 0.725\n",
      "chk\n",
      "epoch 2, loss: 0.828, te_loss: 0.726, acc: 0.783, te acc: 0.838\n",
      "chk\n",
      "epoch 3, loss: 0.624, te_loss: 0.547, acc: 0.828, te acc: 0.900\n",
      "chk\n",
      "epoch 4, loss: 0.493, te_loss: 0.467, acc: 0.869, te acc: 0.887\n",
      "epoch 5, loss: 0.377, te_loss: 0.404, acc: 0.889, te acc: 0.887\n",
      "epoch 6, loss: 0.292, te_loss: 0.314, acc: 0.936, te acc: 0.912\n",
      "chk\n",
      "epoch 7, loss: 0.207, te_loss: 0.277, acc: 0.966, te acc: 0.925\n",
      "chk\n",
      "epoch 8, loss: 0.173, te_loss: 0.227, acc: 0.967, te acc: 0.950\n",
      "chk\n",
      "epoch 9, loss: 0.128, te_loss: 0.208, acc: 0.981, te acc: 0.925\n",
      "epoch 10, loss: 0.111, te_loss: 0.188, acc: 0.983, te acc: 0.938\n",
      "epoch 11, loss: 0.095, te_loss: 0.197, acc: 0.984, te acc: 0.925\n",
      "epoch 12, loss: 0.063, te_loss: 0.184, acc: 0.992, te acc: 0.950\n",
      "epoch 13, loss: 0.054, te_loss: 0.144, acc: 1.000, te acc: 0.963\n",
      "chk\n",
      "epoch 14, loss: 0.046, te_loss: 0.155, acc: 0.995, te acc: 0.938\n",
      "epoch 15, loss: 0.042, te_loss: 0.145, acc: 0.997, te acc: 0.938\n",
      "epoch 16, loss: 0.033, te_loss: 0.149, acc: 1.000, te acc: 0.925\n",
      "epoch 17, loss: 0.027, te_loss: 0.130, acc: 1.000, te acc: 0.938\n",
      "epoch 18, loss: 0.024, te_loss: 0.133, acc: 1.000, te acc: 0.938\n",
      "epoch 19, loss: 0.022, te_loss: 0.148, acc: 1.000, te acc: 0.938\n",
      "epoch 20, loss: 0.016, te_loss: 0.138, acc: 1.000, te acc: 0.938\n",
      "epoch 21, loss: 0.018, te_loss: 0.129, acc: 1.000, te acc: 0.938\n",
      "epoch 22, loss: 0.013, te_loss: 0.120, acc: 1.000, te acc: 0.938\n",
      "epoch 23, loss: 0.011, te_loss: 0.132, acc: 1.000, te acc: 0.950\n",
      "epoch 24, loss: 0.013, te_loss: 0.121, acc: 1.000, te acc: 0.938\n",
      "epoch 25, loss: 0.011, te_loss: 0.120, acc: 1.000, te acc: 0.938\n",
      "epoch 26, loss: 0.010, te_loss: 0.130, acc: 1.000, te acc: 0.950\n",
      "epoch 27, loss: 0.010, te_loss: 0.120, acc: 1.000, te acc: 0.938\n",
      "epoch 28, loss: 0.009, te_loss: 0.120, acc: 1.000, te acc: 0.950\n",
      "epoch 29, loss: 0.008, te_loss: 0.130, acc: 1.000, te acc: 0.950\n",
      "epoch 30, loss: 0.009, te_loss: 0.113, acc: 1.000, te acc: 0.938\n",
      "epoch 31, loss: 0.007, te_loss: 0.113, acc: 1.000, te acc: 0.963\n",
      "epoch 32, loss: 0.007, te_loss: 0.121, acc: 1.000, te acc: 0.938\n",
      "epoch 33, loss: 0.007, te_loss: 0.118, acc: 1.000, te acc: 0.950\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.276, acc: 0.000, te acc: 0.900\n",
      "epoch 1, loss: 0.000, te_loss: 0.276, acc: 0.000, te acc: 0.900\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.624, te_loss: 1.423, acc: 0.386, te acc: 0.650\n",
      "chk\n",
      "epoch 1, loss: 1.193, te_loss: 1.108, acc: 0.641, te acc: 0.725\n",
      "chk\n",
      "epoch 2, loss: 0.869, te_loss: 0.904, acc: 0.764, te acc: 0.738\n",
      "chk\n",
      "epoch 3, loss: 0.657, te_loss: 0.754, acc: 0.833, te acc: 0.787\n",
      "chk\n",
      "epoch 4, loss: 0.506, te_loss: 0.652, acc: 0.883, te acc: 0.787\n",
      "epoch 5, loss: 0.395, te_loss: 0.562, acc: 0.894, te acc: 0.838\n",
      "chk\n",
      "epoch 6, loss: 0.285, te_loss: 0.484, acc: 0.944, te acc: 0.838\n",
      "epoch 7, loss: 0.223, te_loss: 0.444, acc: 0.939, te acc: 0.863\n",
      "chk\n",
      "epoch 8, loss: 0.180, te_loss: 0.451, acc: 0.967, te acc: 0.838\n",
      "epoch 9, loss: 0.141, te_loss: 0.416, acc: 0.975, te acc: 0.850\n",
      "epoch 10, loss: 0.110, te_loss: 0.369, acc: 0.981, te acc: 0.863\n",
      "epoch 11, loss: 0.087, te_loss: 0.375, acc: 0.984, te acc: 0.863\n",
      "epoch 12, loss: 0.070, te_loss: 0.388, acc: 0.995, te acc: 0.812\n",
      "epoch 13, loss: 0.057, te_loss: 0.324, acc: 0.997, te acc: 0.887\n",
      "chk\n",
      "epoch 14, loss: 0.048, te_loss: 0.376, acc: 0.997, te acc: 0.863\n",
      "epoch 15, loss: 0.047, te_loss: 0.343, acc: 0.994, te acc: 0.850\n",
      "epoch 16, loss: 0.036, te_loss: 0.345, acc: 0.997, te acc: 0.875\n",
      "epoch 17, loss: 0.029, te_loss: 0.370, acc: 0.998, te acc: 0.863\n",
      "epoch 18, loss: 0.030, te_loss: 0.343, acc: 0.998, te acc: 0.825\n",
      "epoch 19, loss: 0.021, te_loss: 0.337, acc: 1.000, te acc: 0.863\n",
      "epoch 20, loss: 0.021, te_loss: 0.346, acc: 1.000, te acc: 0.863\n",
      "epoch 21, loss: 0.018, te_loss: 0.375, acc: 1.000, te acc: 0.838\n",
      "epoch 22, loss: 0.017, te_loss: 0.344, acc: 1.000, te acc: 0.838\n",
      "epoch 23, loss: 0.017, te_loss: 0.338, acc: 1.000, te acc: 0.875\n",
      "epoch 24, loss: 0.012, te_loss: 0.382, acc: 1.000, te acc: 0.838\n",
      "epoch 25, loss: 0.014, te_loss: 0.359, acc: 1.000, te acc: 0.850\n",
      "epoch 26, loss: 0.012, te_loss: 0.332, acc: 1.000, te acc: 0.863\n",
      "epoch 27, loss: 0.011, te_loss: 0.339, acc: 1.000, te acc: 0.863\n",
      "epoch 28, loss: 0.009, te_loss: 0.328, acc: 1.000, te acc: 0.863\n",
      "epoch 29, loss: 0.011, te_loss: 0.323, acc: 0.998, te acc: 0.875\n",
      "epoch 30, loss: 0.009, te_loss: 0.352, acc: 1.000, te acc: 0.850\n",
      "epoch 31, loss: 0.008, te_loss: 0.316, acc: 1.000, te acc: 0.863\n",
      "epoch 32, loss: 0.007, te_loss: 0.318, acc: 1.000, te acc: 0.863\n",
      "epoch 33, loss: 0.007, te_loss: 0.337, acc: 1.000, te acc: 0.838\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.336, acc: 0.000, te acc: 0.875\n",
      "epoch 1, loss: 0.000, te_loss: 0.336, acc: 0.000, te acc: 0.875\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.612, te_loss: 1.405, acc: 0.403, te acc: 0.637\n",
      "chk\n",
      "epoch 1, loss: 1.123, te_loss: 1.052, acc: 0.697, te acc: 0.713\n",
      "chk\n",
      "epoch 2, loss: 0.822, te_loss: 0.828, acc: 0.780, te acc: 0.725\n",
      "chk\n",
      "epoch 3, loss: 0.626, te_loss: 0.694, acc: 0.836, te acc: 0.812\n",
      "chk\n",
      "epoch 4, loss: 0.510, te_loss: 0.609, acc: 0.833, te acc: 0.787\n",
      "epoch 5, loss: 0.385, te_loss: 0.531, acc: 0.905, te acc: 0.825\n",
      "chk\n",
      "epoch 6, loss: 0.287, te_loss: 0.477, acc: 0.938, te acc: 0.850\n",
      "chk\n",
      "epoch 7, loss: 0.205, te_loss: 0.433, acc: 0.966, te acc: 0.838\n",
      "epoch 8, loss: 0.159, te_loss: 0.377, acc: 0.973, te acc: 0.863\n",
      "chk\n",
      "epoch 9, loss: 0.135, te_loss: 0.339, acc: 0.970, te acc: 0.875\n",
      "chk\n",
      "epoch 10, loss: 0.109, te_loss: 0.355, acc: 0.983, te acc: 0.887\n",
      "chk\n",
      "epoch 11, loss: 0.077, te_loss: 0.327, acc: 0.992, te acc: 0.900\n",
      "chk\n",
      "epoch 12, loss: 0.054, te_loss: 0.304, acc: 0.997, te acc: 0.900\n",
      "epoch 13, loss: 0.048, te_loss: 0.311, acc: 0.997, te acc: 0.887\n",
      "epoch 14, loss: 0.044, te_loss: 0.328, acc: 0.997, te acc: 0.863\n",
      "epoch 15, loss: 0.041, te_loss: 0.341, acc: 0.995, te acc: 0.887\n",
      "epoch 16, loss: 0.032, te_loss: 0.309, acc: 1.000, te acc: 0.887\n",
      "epoch 17, loss: 0.028, te_loss: 0.307, acc: 1.000, te acc: 0.887\n",
      "epoch 18, loss: 0.020, te_loss: 0.304, acc: 1.000, te acc: 0.887\n",
      "epoch 19, loss: 0.021, te_loss: 0.321, acc: 0.998, te acc: 0.887\n",
      "epoch 20, loss: 0.017, te_loss: 0.303, acc: 1.000, te acc: 0.887\n",
      "epoch 21, loss: 0.019, te_loss: 0.312, acc: 1.000, te acc: 0.900\n",
      "epoch 22, loss: 0.012, te_loss: 0.313, acc: 1.000, te acc: 0.900\n",
      "epoch 23, loss: 0.012, te_loss: 0.311, acc: 1.000, te acc: 0.912\n",
      "chk\n",
      "epoch 24, loss: 0.014, te_loss: 0.310, acc: 1.000, te acc: 0.887\n",
      "epoch 25, loss: 0.011, te_loss: 0.313, acc: 1.000, te acc: 0.900\n",
      "epoch 26, loss: 0.010, te_loss: 0.314, acc: 1.000, te acc: 0.900\n",
      "epoch 27, loss: 0.008, te_loss: 0.299, acc: 1.000, te acc: 0.900\n",
      "epoch 28, loss: 0.008, te_loss: 0.290, acc: 1.000, te acc: 0.887\n",
      "epoch 29, loss: 0.008, te_loss: 0.311, acc: 1.000, te acc: 0.912\n",
      "epoch 30, loss: 0.007, te_loss: 0.310, acc: 1.000, te acc: 0.900\n",
      "epoch 31, loss: 0.007, te_loss: 0.308, acc: 1.000, te acc: 0.900\n",
      "epoch 32, loss: 0.008, te_loss: 0.303, acc: 1.000, te acc: 0.900\n",
      "epoch 33, loss: 0.006, te_loss: 0.306, acc: 1.000, te acc: 0.900\n",
      "epoch 34, loss: 0.006, te_loss: 0.319, acc: 1.000, te acc: 0.912\n",
      "epoch 35, loss: 0.005, te_loss: 0.315, acc: 1.000, te acc: 0.900\n",
      "epoch 36, loss: 0.005, te_loss: 0.298, acc: 1.000, te acc: 0.900\n",
      "epoch 37, loss: 0.007, te_loss: 0.332, acc: 1.000, te acc: 0.912\n",
      "epoch 38, loss: 0.005, te_loss: 0.308, acc: 1.000, te acc: 0.912\n",
      "epoch 39, loss: 0.004, te_loss: 0.305, acc: 1.000, te acc: 0.912\n",
      "epoch 40, loss: 0.004, te_loss: 0.296, acc: 1.000, te acc: 0.912\n",
      "epoch 41, loss: 0.003, te_loss: 0.316, acc: 1.000, te acc: 0.912\n",
      "epoch 42, loss: 0.003, te_loss: 0.309, acc: 1.000, te acc: 0.900\n",
      "epoch 43, loss: 0.003, te_loss: 0.320, acc: 1.000, te acc: 0.912\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.203, acc: 0.000, te acc: 0.887\n",
      "epoch 1, loss: 0.000, te_loss: 0.203, acc: 0.000, te acc: 0.887\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.644, te_loss: 1.409, acc: 0.377, te acc: 0.450\n",
      "chk\n",
      "epoch 1, loss: 1.224, te_loss: 1.080, acc: 0.575, te acc: 0.675\n",
      "chk\n",
      "epoch 2, loss: 0.903, te_loss: 0.832, acc: 0.744, te acc: 0.787\n",
      "chk\n",
      "epoch 3, loss: 0.667, te_loss: 0.663, acc: 0.822, te acc: 0.838\n",
      "chk\n",
      "epoch 4, loss: 0.515, te_loss: 0.571, acc: 0.870, te acc: 0.787\n",
      "epoch 5, loss: 0.389, te_loss: 0.497, acc: 0.912, te acc: 0.825\n",
      "epoch 6, loss: 0.307, te_loss: 0.453, acc: 0.925, te acc: 0.863\n",
      "chk\n",
      "epoch 7, loss: 0.228, te_loss: 0.400, acc: 0.947, te acc: 0.850\n",
      "epoch 8, loss: 0.185, te_loss: 0.371, acc: 0.961, te acc: 0.850\n",
      "epoch 9, loss: 0.146, te_loss: 0.339, acc: 0.973, te acc: 0.863\n",
      "epoch 10, loss: 0.114, te_loss: 0.356, acc: 0.980, te acc: 0.838\n",
      "epoch 11, loss: 0.093, te_loss: 0.313, acc: 0.989, te acc: 0.863\n",
      "epoch 12, loss: 0.077, te_loss: 0.307, acc: 0.992, te acc: 0.850\n",
      "epoch 13, loss: 0.064, te_loss: 0.318, acc: 0.992, te acc: 0.863\n",
      "epoch 14, loss: 0.049, te_loss: 0.327, acc: 0.997, te acc: 0.875\n",
      "chk\n",
      "epoch 15, loss: 0.042, te_loss: 0.295, acc: 0.998, te acc: 0.863\n",
      "epoch 16, loss: 0.038, te_loss: 0.312, acc: 0.997, te acc: 0.875\n",
      "epoch 17, loss: 0.032, te_loss: 0.288, acc: 0.998, te acc: 0.863\n",
      "epoch 18, loss: 0.026, te_loss: 0.308, acc: 0.998, te acc: 0.875\n",
      "epoch 19, loss: 0.022, te_loss: 0.284, acc: 0.998, te acc: 0.875\n",
      "epoch 20, loss: 0.020, te_loss: 0.287, acc: 1.000, te acc: 0.875\n",
      "epoch 21, loss: 0.018, te_loss: 0.290, acc: 1.000, te acc: 0.863\n",
      "epoch 22, loss: 0.017, te_loss: 0.283, acc: 0.998, te acc: 0.863\n",
      "epoch 23, loss: 0.015, te_loss: 0.296, acc: 1.000, te acc: 0.850\n",
      "epoch 24, loss: 0.014, te_loss: 0.293, acc: 1.000, te acc: 0.863\n",
      "epoch 25, loss: 0.012, te_loss: 0.282, acc: 1.000, te acc: 0.863\n",
      "epoch 26, loss: 0.010, te_loss: 0.272, acc: 1.000, te acc: 0.875\n",
      "epoch 27, loss: 0.011, te_loss: 0.280, acc: 1.000, te acc: 0.875\n",
      "epoch 28, loss: 0.011, te_loss: 0.301, acc: 1.000, te acc: 0.863\n",
      "epoch 29, loss: 0.008, te_loss: 0.284, acc: 1.000, te acc: 0.875\n",
      "epoch 30, loss: 0.008, te_loss: 0.302, acc: 1.000, te acc: 0.875\n",
      "epoch 31, loss: 0.006, te_loss: 0.300, acc: 1.000, te acc: 0.875\n",
      "epoch 32, loss: 0.008, te_loss: 0.320, acc: 1.000, te acc: 0.875\n",
      "epoch 33, loss: 0.008, te_loss: 0.281, acc: 1.000, te acc: 0.875\n",
      "epoch 34, loss: 0.006, te_loss: 0.293, acc: 1.000, te acc: 0.875\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.278, acc: 0.000, te acc: 0.925\n",
      "epoch 1, loss: 0.000, te_loss: 0.278, acc: 0.000, te acc: 0.925\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n",
      "X_tr shape (640, 4048)\n",
      "X_te shape (80, 4048)\n",
      "X_val shape (80, 4048)\n",
      "is 4048\n",
      "epoch 0, loss: 1.629, te_loss: 1.364, acc: 0.330, te acc: 0.688\n",
      "chk\n",
      "epoch 1, loss: 1.165, te_loss: 0.971, acc: 0.650, te acc: 0.787\n",
      "chk\n",
      "epoch 2, loss: 0.870, te_loss: 0.735, acc: 0.762, te acc: 0.838\n",
      "chk\n",
      "epoch 3, loss: 0.645, te_loss: 0.584, acc: 0.850, te acc: 0.850\n",
      "chk\n",
      "epoch 4, loss: 0.497, te_loss: 0.474, acc: 0.880, te acc: 0.825\n",
      "epoch 5, loss: 0.381, te_loss: 0.395, acc: 0.914, te acc: 0.850\n",
      "epoch 6, loss: 0.288, te_loss: 0.339, acc: 0.928, te acc: 0.900\n",
      "chk\n",
      "epoch 7, loss: 0.228, te_loss: 0.305, acc: 0.944, te acc: 0.912\n",
      "chk\n",
      "epoch 8, loss: 0.168, te_loss: 0.272, acc: 0.977, te acc: 0.875\n",
      "epoch 9, loss: 0.136, te_loss: 0.259, acc: 0.972, te acc: 0.887\n",
      "epoch 10, loss: 0.103, te_loss: 0.225, acc: 0.984, te acc: 0.900\n",
      "epoch 11, loss: 0.081, te_loss: 0.209, acc: 0.994, te acc: 0.925\n",
      "chk\n",
      "epoch 12, loss: 0.061, te_loss: 0.190, acc: 0.994, te acc: 0.925\n",
      "epoch 13, loss: 0.050, te_loss: 0.175, acc: 0.997, te acc: 0.925\n",
      "epoch 14, loss: 0.051, te_loss: 0.176, acc: 0.995, te acc: 0.912\n",
      "epoch 15, loss: 0.042, te_loss: 0.173, acc: 0.995, te acc: 0.925\n",
      "epoch 16, loss: 0.031, te_loss: 0.176, acc: 0.998, te acc: 0.912\n",
      "epoch 17, loss: 0.031, te_loss: 0.157, acc: 0.998, te acc: 0.925\n",
      "epoch 18, loss: 0.030, te_loss: 0.148, acc: 0.997, te acc: 0.925\n",
      "epoch 19, loss: 0.021, te_loss: 0.149, acc: 1.000, te acc: 0.925\n",
      "epoch 20, loss: 0.022, te_loss: 0.144, acc: 0.998, te acc: 0.938\n",
      "chk\n",
      "epoch 21, loss: 0.016, te_loss: 0.145, acc: 1.000, te acc: 0.925\n",
      "epoch 22, loss: 0.016, te_loss: 0.134, acc: 1.000, te acc: 0.938\n",
      "epoch 23, loss: 0.015, te_loss: 0.135, acc: 1.000, te acc: 0.938\n",
      "epoch 24, loss: 0.016, te_loss: 0.181, acc: 0.998, te acc: 0.912\n",
      "epoch 25, loss: 0.013, te_loss: 0.128, acc: 1.000, te acc: 0.950\n",
      "chk\n",
      "epoch 26, loss: 0.009, te_loss: 0.118, acc: 1.000, te acc: 0.963\n",
      "chk\n",
      "epoch 27, loss: 0.010, te_loss: 0.125, acc: 1.000, te acc: 0.950\n",
      "epoch 28, loss: 0.008, te_loss: 0.129, acc: 1.000, te acc: 0.938\n",
      "epoch 29, loss: 0.007, te_loss: 0.127, acc: 1.000, te acc: 0.938\n",
      "epoch 30, loss: 0.007, te_loss: 0.118, acc: 1.000, te acc: 0.950\n",
      "epoch 31, loss: 0.009, te_loss: 0.120, acc: 1.000, te acc: 0.938\n",
      "epoch 32, loss: 0.007, te_loss: 0.133, acc: 1.000, te acc: 0.925\n",
      "epoch 33, loss: 0.006, te_loss: 0.123, acc: 1.000, te acc: 0.938\n",
      "epoch 34, loss: 0.005, te_loss: 0.131, acc: 1.000, te acc: 0.938\n",
      "epoch 35, loss: 0.005, te_loss: 0.122, acc: 1.000, te acc: 0.925\n",
      "epoch 36, loss: 0.005, te_loss: 0.121, acc: 1.000, te acc: 0.925\n",
      "epoch 37, loss: 0.006, te_loss: 0.125, acc: 1.000, te acc: 0.938\n",
      "epoch 38, loss: 0.005, te_loss: 0.122, acc: 1.000, te acc: 0.925\n",
      "epoch 39, loss: 0.004, te_loss: 0.119, acc: 1.000, te acc: 0.950\n",
      "epoch 40, loss: 0.004, te_loss: 0.119, acc: 1.000, te acc: 0.938\n",
      "epoch 41, loss: 0.004, te_loss: 0.117, acc: 1.000, te acc: 0.950\n",
      "epoch 42, loss: 0.003, te_loss: 0.120, acc: 1.000, te acc: 0.950\n",
      "epoch 43, loss: 0.004, te_loss: 0.109, acc: 1.000, te acc: 0.950\n",
      "epoch 44, loss: 0.004, te_loss: 0.109, acc: 1.000, te acc: 0.950\n",
      "epoch 45, loss: 0.003, te_loss: 0.122, acc: 1.000, te acc: 0.938\n",
      "epoch 46, loss: 0.004, te_loss: 0.120, acc: 1.000, te acc: 0.950\n",
      "using the best model, which was copied\n",
      "epoch 0, loss: 0.000, te_loss: 0.357, acc: 0.000, te acc: 0.838\n",
      "epoch 1, loss: 0.000, te_loss: 0.357, acc: 0.000, te acc: 0.838\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([32, 4048])\n",
      "torch.Size([16, 4048])\n"
     ]
    }
   ],
   "source": [
    "n_class = len(set(Y))\n",
    "from collections import Counter\n",
    "y_ct =dict(Counter(Y))\n",
    "minct = np.inf\n",
    "for k, v in y_ct.items():\n",
    "    minct = min(minct, v)\n",
    "print('minct', minct)\n",
    "cv_accs = []\n",
    "all_preds, all_gts = [], []\n",
    "\n",
    "for state in range(1):\n",
    "    skf = StratifiedKFold(10, shuffle=True, random_state=state)\n",
    "    for cv, (train, val) in enumerate(skf.split(X[:800], Y[:800])):\n",
    "#         print(len(train), len(test))\n",
    "\n",
    "        X_s, X_val = X_new[train], X_new[val]\n",
    "        Y_s, Y_val = Y[train], Y[val]\n",
    "        skf2 = StratifiedKFold(9, shuffle=True, random_state=state)\n",
    "        for train_final, test_final in skf2.split(X_s, Y_s):\n",
    "            break\n",
    "            \n",
    "        for t in test_final: \n",
    "            assert not t in train_final\n",
    "        train = train_final\n",
    "        X_te = X_s[test_final]\n",
    "        Y_te = Y_s[test_final]\n",
    "        X_tr = X_s[train_final]\n",
    "        Y_tr = Y_s[train_final]\n",
    "  \n",
    "        print('X_tr shape', X_tr.shape)\n",
    "        print('X_te shape', X_te.shape)\n",
    "        print('X_val shape', X_val.shape)\n",
    "    #     break\n",
    "\n",
    "        model = MLP(512, X_tr.shape[-1], len(set(Y)), 0.6, 2)\n",
    "\n",
    "        import bravo_ml\n",
    "\n",
    "        from bravo_ml.modelling.torch_trainers import train_classifier\n",
    "\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        import copy\n",
    "\n",
    "        train_dset = TensorDataset(torch.from_numpy(X_tr.copy()), torch.from_numpy(Y_tr.copy()))\n",
    "        test_dset = TensorDataset(torch.from_numpy(X_te.copy()), torch.from_numpy(Y_te.copy()))\n",
    "        val_dset = TensorDataset(torch.from_numpy(X_val.copy()), torch.from_numpy(Y_val.copy()))\n",
    "\n",
    "        train_loader = DataLoader(train_dset, batch_size=args['bs'], shuffle=True) \n",
    "        test_loader = DataLoader(test_dset, batch_size=args['bs'], shuffle=False)\n",
    "        val_loader = DataLoader(val_dset, batch_size=args['bs'], shuffle=False)\n",
    "\n",
    "        m, hist =train_classifier(model, train_loader,\n",
    "                        val_loader, \n",
    "                        'cpu', \n",
    "                        patience=20, \n",
    "                         wandb_flag=False,\n",
    "                        lr=args['lr'],\n",
    "                                 es_metric='acc',\n",
    "                                 checkpoint=True, \n",
    "                                 file_checkpoint=False)\n",
    "        \n",
    "        # Can save the model if you want using line below.\n",
    "#         torch.save(m.state_dict(), './models/expressions/model_%d.pth' %cv)\n",
    "        m, hist_final = train_classifier(m, \n",
    "                                        train_loader,\n",
    "                                        test_loader,\n",
    "                                        'cpu',\n",
    "                                        patience=1,\n",
    "                                        wandb_flag=False,\n",
    "                                        lr=args['lr'], \n",
    "                                        run_training=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                print(x.shape)\n",
    "                preds = model(x.float())\n",
    "                ml = preds.argmax(1).detach().cpu().numpy()\n",
    "                all_preds.extend(ml)\n",
    "                all_gts.extend(y.detach().cpu().numpy())\n",
    "        cv_accs.append(np.max(hist_final['val_acc']))\n",
    "    \n",
    "# wandb.log({'cvaccs':cv_accs,\n",
    "#           'meanacc':np.mean(cv_accs)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8825"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(x, B=2000, metric=np.median):\n",
    "    dist = [metric(np.random.choice(x, size=len(x), replace=True)) for _ in range(B)]\n",
    "    return dist\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "def return_distribution(lom, metric=np.median):\n",
    "    \"\"\"\n",
    "    Input : list of metrics - basically just like a list, we want to change it to be median and stuff. \n",
    "    \"\"\"\n",
    "    return bootstrap(lom, len(lom), metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.25 99% CI [85.88, 90.5]\n"
     ]
    }
   ],
   "source": [
    "print('%.2f' %(100*np.mean(cv_accs)), '99% CI', [np.round(k*100, 2) for k in np.percentile(bootstrap(cv_accs, metric=np.mean), [.5, 99.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 2, 512)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Lips\\nback', 'Lips\\nforward',\n",
    " 'Mouth\\nclosed', 'Mouth\\nopen',\n",
    " 'Tongue\\ndown' ,'Tongue\\nup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm =np.zeros((len(labels), len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lips\\nback',\n",
       " 'Lips\\nforward',\n",
       " 'Mouth\\nclosed',\n",
       " 'Mouth\\nopen',\n",
       " 'Tongue\\ndown',\n",
       " 'Tongue\\nup']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, (g,p) in enumerate(zip(all_gts, all_preds)):\n",
    "    cm[g, p] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(cm.shape[0]):\n",
    "    cm[k, :] = cm[k, :]/np.sum(cm[k, :])\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(5, 4))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "sns.heatmap(pd.DataFrame(data=cm,columns=labels, index=labels), cmap='bone')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Avatar PC control')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/avatar_pc_cm.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=cm,columns=labels, index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('./results/pc_cm.h5', key='df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silent_spelling",
   "language": "python",
   "name": "silent_spelling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
