# Avatar decoding

Here we train and evaluate a model that can decode 6 orofacial movements from brain activity. 

These movements were then fed into an avatar to animate facial movement from brain activity. 
 
## Instructions 

Install the environment and packages per the `README.MD` file in the `text` folder, then try out the decoding for yourself in
`avatar_orofacial_classifier.ipynb`

