{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text model training for the paper \"A high-performance neuroprosthesis for speech decoding and avatar control\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/server/path/torchaud/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version 1.12.0\n",
      "torch audio version 0.12.0\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from os.path import join\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchaudio.models import decoder\n",
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "\n",
    "\n",
    "print('torch version', torch.__version__)\n",
    "print('torch audio version', torchaudio.__version__)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from data_loading_utilities.normalize import normalize\n",
    "import copy\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 23 06:33:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN V      Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 28%   34C    P2    35W / 250W |   3076MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN V      Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 28%   32C    P8    24W / 250W |   3143MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA TITAN V      Off  | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 28%   30C    P8    24W / 250W |   2173MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3691569      C   ...ur_working_env/bin/python     1685MiB |\n",
      "|    0   N/A  N/A   3700637      C   ...ur_working_env/bin/python     1387MiB |\n",
      "|    1   N/A  N/A    390057      C   ...nda3/envs/ecog/bin/python     3139MiB |\n",
      "|    2   N/A  N/A    459859      C   ...silva/py3.9env/bin/python     2169MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Load a GPU and modify this to be your directories!!!!\n",
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)\n",
    "curdir = './' # TODO: Change to your current directory\n",
    "data_dir = './data' # TODO: Change to where the data is stored. \n",
    "device = 'cuda' # Set to cpu if you dont have a gpu avaiable! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--low_density'], dest='low_density', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='downsample the grid', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the experiment, can change the hyperparameters as you see fit or edit for your own models\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--decimation', \n",
    "                   default=6, \n",
    "                   type=int, \n",
    "                   help='How much to downsample neural data')\n",
    "parser.add_argument('--hidden_dim',\n",
    "                    type=int,\n",
    "                   default=256,\n",
    "                   help=\"how many hid.units in model\")\n",
    "parser.add_argument('--lr', \n",
    "                    type=float,\n",
    "                   default=1e-3,\n",
    "                   help='learning rate')\n",
    "parser.add_argument('--ks', \n",
    "                    type=int,\n",
    "                   default=2,\n",
    "                   help='ks of input conv')\n",
    "parser.add_argument('--num_layers',\n",
    "                   type=int, \n",
    "                   default=4,\n",
    "                   help='number of layers')\n",
    "parser.add_argument('--dropout', \n",
    "                   type=float, \n",
    "                   default=0.5, \n",
    "                   help='dropout amount')\n",
    "parser.add_argument('--feat_stream', \n",
    "                   type=str, \n",
    "                   default='both',\n",
    "                   help='which stream. both, hga, or raw')\n",
    "parser.add_argument('--bs',\n",
    "                   type=int, \n",
    "                   default=64, \n",
    "                   help='batch size')\n",
    "parser.add_argument('--smooth',\n",
    "                   type=int,\n",
    "                   default=0)\n",
    "parser.add_argument('--no_normalize', \n",
    "                   action='store_false',\n",
    "                   help='If you add')\n",
    "parser.add_argument('--LM_WEIGHT', \n",
    "                   help='how much the LM is weighted during beam search', \n",
    "                   type=float, \n",
    "                   default=3.23)\n",
    "parser.add_argument('--WORD_SCORE', \n",
    "                   help='word insertion score for beam',\n",
    "                    type=float,\n",
    "                    default=-.26\n",
    "                   )\n",
    "parser.add_argument('--beam_width', \n",
    "                   help='beam size to use',\n",
    "                   type=int,\n",
    "                   default=100)\n",
    "parser.add_argument('--checkpoint_dir',\n",
    "                   help='where 2 load model',\n",
    "                   type=str,\n",
    "                   default=None)\n",
    "parser.add_argument('--feedforward', \n",
    "                   help='no bidirectional',\n",
    "                   action='store_true')\n",
    "parser.add_argument('--pretrained',\n",
    "                   help='pretrained model',\n",
    "                   type=str,\n",
    "                   default=None)\n",
    "parser.add_argument('--train_amt',\n",
    "                   help='amt of train data',\n",
    "                   type=float, \n",
    "                    default=1.0)\n",
    "parser.add_argument('--samples_to_trim',\n",
    "                   help='num samps back to go',\n",
    "                   default=0, \n",
    "                   type=int)\n",
    "parser.add_argument('--ndense',\n",
    "                   help='transfer dense',\n",
    "                   default=40,\n",
    "                   type=int)\n",
    "parser.add_argument('--transfer_audio', \n",
    "                   help='true if transfer audio. then switch conv', \n",
    "                   action='store_true')\n",
    "\n",
    "parser.add_argument('--num_50', \n",
    "                   help='how many of the 50 phrase datapoints to use, so to not over fit', \n",
    "                   type=int,\n",
    "                   default=500)\n",
    "parser.add_argument('--num_500', \n",
    "                   help='how much 500 phrase data to use',\n",
    "                   type=int,\n",
    "                   default=None)\n",
    "parser.add_argument('--eval_set', \n",
    "                   help='which eval set to use, for CV', \n",
    "                   type=int,\n",
    "                   default=None)\n",
    "\n",
    "parser.add_argument('--model_type', \n",
    "                    help='which model to use',\n",
    "                   type=str,\n",
    "                   default='cnnrnn')\n",
    "\n",
    "#### NEW ARGS FOR CRDNN\n",
    "parser.add_argument('--KS2', \n",
    "                   help='kernel size for second conv',\n",
    "                    type=int,\n",
    "                   default=2)\n",
    "parser.add_argument('--stride1',\n",
    "                   help='stride, conv1',\n",
    "                   type=int,\n",
    "                   default=1)\n",
    "parser.add_argument('--stride2', \n",
    "                   help='stride, conv2',\n",
    "                   type=int,\n",
    "                   default=2)\n",
    "\n",
    "parser.add_argument('--jitter_amt', \n",
    "                    help='how much 2 jit',\n",
    "                    type=float, \n",
    "                    default=0.5)\n",
    "\n",
    "parser.add_argument('--chan_noise', \n",
    "                    help='how much noise', \n",
    "                    type=float,\n",
    "                    default=0.04)\n",
    "\n",
    "parser.add_argument('--blackout_prob', \n",
    "                    help='odds of blackout for data augmentation', \n",
    "                    type=float, \n",
    "                    default=0.05)\n",
    "parser.add_argument('--weight_decay', \n",
    "                    help='weight decay to apply to weights', \n",
    "                    type=float,\n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--printall', \n",
    "                   help='print every pred during evaluation, useful to see all predictions. ',\n",
    "                   action='store_true')\n",
    "\n",
    "parser.add_argument('--word_ct_weight',\n",
    "                   help='how much to weight the word count loss',\n",
    "                   type=float, \n",
    "                   default=0.0)\n",
    "\n",
    "parser.add_argument('--clipamt',\n",
    "                   help='how much to clip gradient',\n",
    "                   type=float,\n",
    "                   default=1.0)\n",
    "\n",
    "parser.add_argument('--winstart', \n",
    "                   help='how much time before go-cue to use during model training',\n",
    "                   type=float, \n",
    "                   default=0.0)\n",
    "parser.add_argument('--winend',\n",
    "                   help='how much time after go cue to use during model training/eval',\n",
    "                   type=float,\n",
    "                   default=7.5)\n",
    "\n",
    "parser.add_argument('--normalization_strategy', \n",
    "                    help='how to normalize the data',\n",
    "                    type=str, \n",
    "                    default='typical')\n",
    "parser.add_argument('--area_to_ablate',\n",
    "                    help='which area to hold out',\n",
    "                    default = None)\n",
    "parser.add_argument('--low_density',\n",
    "                    help='downsample the grid',\n",
    "                    action='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <modified: text/html; unchanged: text/plain>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/userdata/smetzger/repos/pub_code/text/wandb/run-20230823_063305-fq28gqex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/server/path/repos/pub_code/text/wandb/run-20230823_054359-h6gmj97w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <modified: text/html; unchanged: text/plain>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/slm/pub_code/runs/fq28gqex' target=\"_blank\">solar-lake-21</a></strong> to <a href='https://wandb.ai/slm/pub_code' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/slm/pub_code' target=\"_blank\">https://wandb.ai/slm/pub_code</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/slm/pub_code/runs/fq28gqex' target=\"_blank\">https://wandb.ai/slm/pub_code/runs/fq28gqex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/slm/pub_code/runs/fq28gqex?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fec7684f940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = vars(parser.parse_args('--jitter_amt 1 --chan_noise .0 --weight_decay 1e-05 --LM_WEIGHT 4 --train_amt 1 --hidden_dim 500 --ks 4 --dropout 0.4 --num_layers 3 --eval_set 1 --decimation 6 --word_ct_weight 0.0 --clipamt .0001 --winstart -0.5 --normalization_strategy norm_times'.split()))\n",
    "wandb.init(project='pub_code', \n",
    "          config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load in the labels first and take a look. Its saved as a dataframe. \n",
    "# key values are the ph_label  - phone label\n",
    "# and txt_label, the text\n",
    "labels = pd.read_hdf(join(data_dir, 'training_labels.h5'))\n",
    "labels_te = pd.read_hdf(join(data_dir, 'realtime_test_labels.h5'))\n",
    "\n",
    "# Check no test sents in training data :D \n",
    "for l in labels_te['txt_label'].values: \n",
    "    assert not l in labels['txt_label'].values\n",
    "    \n",
    "labels = pd.concat((labels, labels_te), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples 9506\n",
      "test samples 249\n"
     ]
    }
   ],
   "source": [
    "# Next lets load in the neural data. \n",
    "import os\n",
    "if args['decimation'] ==6:\n",
    "    \"\"\"\n",
    "    Here we're loading the predecimated data\n",
    "    \"\"\"\n",
    "    X = np.load(os.path.join(data_dir, f\"train_data.npy\"))\n",
    "    X_te = np.load(os.path.join(data_dir, f\"realtime_test_data.npy\"))\n",
    "\n",
    "X = np.concatenate((X, X_te), axis=0)\n",
    "print('train samples', X.shape[0] - X_te.shape[0]) \n",
    "print('test samples', X_te.shape[0])\n",
    "\n",
    "# Extract the words\n",
    "all_labs = set(labels['txt_label'].values)\n",
    "all_words = []\n",
    "for a in all_labs: \n",
    "    all_words.extend(a.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading_utilities.normalize import * # Laod up the data normalization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final X shape (9755, 300, 506)\n"
     ]
    }
   ],
   "source": [
    "# Lets normalize the data, can use a variety of options. \n",
    "# We just noramlize across time so that the 2 norm across time is equal to 1. \n",
    "if args['normalization_strategy'] == 'typical':\n",
    "    X[:, :, :X.shape[-1]//2] = normalize(X[:, :, :X.shape[-1]//2])\n",
    "    X[:, :, X.shape[-1]//2:] = normalize(X[:, :, X.shape[-1]//2:])\n",
    "elif args['normalization_strategy'] == 'norm_all_at_once':\n",
    "    X= normalize(X)\n",
    "elif args['normalization_strategy'] == 'norm_times':\n",
    "    X = normalize(X, axis=1)\n",
    "elif args['normalization_strategy'] == 'zero_to_one':\n",
    "    X = minmax_scaling(X)\n",
    "elif args['normalization_strategy'] == 'rezscore':\n",
    "    X = rezscore(X)\n",
    "elif args['normalization_strategy'] == 'none': \n",
    "    print('no normalization.')\n",
    "elif args['normalization_strategy'] == 'pertrial_minmax':\n",
    "    X = pertrial_minmax(X)\n",
    "\n",
    "    \n",
    "# Can edit this to be just one feature stream if you'd like to see effect of using hga vs hga + raw.\n",
    "if args['feat_stream'] == 'hga':\n",
    "    X = X[:, :, :X.shape[-1]//2]\n",
    "elif args['feat_stream'] == 'raw':\n",
    "    X = X[:, :, X.shape[-1]//2:]\n",
    "print('final X shape', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we need to prepare labels for the ctc_decoding and decoder trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/server/path/repos/pub_code/text', '/server/path/torchaud/lib/python39.zip', '/server/path/torchaud/lib/python3.9', '/server/path/torchaud/lib/python3.9/lib-dynload', '', '/server/path/torchaud/lib/python3.9/site-packages', '/server/path/repos/gimdecode', '/server/path/repos/realtime', './']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.ctc_decoding import GreedyCTCDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will rely on the torchaudio ctc_decoder module, which you can read more about here: https://pytorch.org/audio/main/tutorials/asr_inference_with_ctc_decoder_tutorial.html\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torchaudio.functional import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading_utilities.clean_labels import clean_labels\n",
    "# Clean up the labels and remove stress marking from phones. \n",
    "labels, all_ph = clean_labels(labels)\n",
    "phone_enc  = {v:k for k,v in enumerate(sorted([a for a in list(set(all_ph)) if not a == '|']))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets set up a lexicon to map from each word to any valid pronounciation. \n",
    "from collections import defaultdict\n",
    "lex = {}\n",
    "\n",
    "for k,v in zip(labels['txt_label'], labels['ph_label']):\n",
    "    if not '|' in v:\n",
    "        lex[k] = (' '.join(v) + ' |')\n",
    "    else: \n",
    "        v  = '_'.join(v)\n",
    "        v = v.split('|')\n",
    "        for kk, vv in zip(k.split(' '), v):\n",
    "            vv = vv.replace('_', ' ').strip() + ' |'\n",
    "            if not kk == '':\n",
    "                lex[kk] = vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example lexicon items\n",
      "i AY |\n",
      "feel F IY L |\n",
      "like L AY K |\n",
      "driving D R AY V IH NG |\n",
      "are AA R |\n",
      "vocabulary size: 1024\n"
     ]
    }
   ],
   "source": [
    "# Write the lexicon to a text file for the ctc decoder, and prepare it. \n",
    "strings = []\n",
    "for k, v in lex.items():\n",
    "    string = k + ' ' + str(v)\n",
    "    strings.append(string)\n",
    "    \n",
    "strings =  [s for s in strings if len(s) > 3]\n",
    "\n",
    "with open(join(curdir, \"for_ctc/lexicon_phrases_1k.txt\"), \"w\") as f:\n",
    "    f.writelines([s+ '\\n' for s in strings])\n",
    "\n",
    "print('example lexicon items')\n",
    "for s in strings[:5]:\n",
    "    print(s)\n",
    "print('vocabulary size:', len(strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the tokens for the ctc decoder. \n",
    "tokens = ['-', '|'] + list(phone_enc.keys())\n",
    "with open(join(curdir, 'for_ctc/tokens_phrases_1k.txt'), 'w') as f:\n",
    "    f.writelines([t + '\\n' for t in tokens])\n",
    "enc_final = {v:k for k,v in enumerate(tokens)} # Use to map phone to y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token lm False\n"
     ]
    }
   ],
   "source": [
    "beam_search_decoder= ctc_decoder(\n",
    "    lexicon = join(curdir, 'for_ctc/lexicon_phrases_1k.txt'),\n",
    "    tokens = join(curdir, 'for_ctc/tokens_phrases_1k.txt'),\n",
    "    lm = join(curdir, 'custom_lms/full_corpus_lm_3_abs_slm.binary'),\n",
    "    nbest=3,\n",
    "    beam_size=args['beam_width'],\n",
    "    lm_weight=args['LM_WEIGHT'],\n",
    "    word_score=args['WORD_SCORE'],\n",
    "    sil_token = '|', \n",
    "    unk_word = '<unk>',\n",
    ")\n",
    "\n",
    "greedy_decoder = GreedyCTCDecoder(tokens)\n",
    "greedy = GreedyCTCDecoder(labels=list(enc_final.keys()))\n",
    "\n",
    "# Prepare neural and target data for CTC loss\n",
    "y_final = []\n",
    "for t, targ in zip(labels['txt_label'], labels['ph_label']):\n",
    "    cur_y = []\n",
    "    cur_y.append(enc_final['|'])\n",
    "    for ph in targ:\n",
    "        cur_y.append(enc_final[ph])\n",
    "    cur_y.append(enc_final['|'])\n",
    "    y_final.append(cur_y)\n",
    "\n",
    "# Pad with -1\n",
    "y_final_ = -1*np.ones((len(y_final), np.max([len(y) for y in y_final])))\n",
    "\n",
    "# Store the lengths\n",
    "targ_lengths =[]\n",
    "for k, y in enumerate(y_final):\n",
    "    y_final_[k, :len(y)] = np.array(y)\n",
    "    targ_lengths.append(len(y))\n",
    "targ_lengths = np.array(targ_lengths)\n",
    "Y = y_final_\n",
    "\n",
    "lens = [(l//args['decimation']) for l in labels['length']] # Adjust lengths based on decimation. \n",
    "# Finalize the lengths. \n",
    "outlens = targ_lengths\n",
    "lens = np.array(lens)\n",
    "lens = lens - args['samples_to_trim']\n",
    "\n",
    "# Adjust lenghts to be correct. \n",
    "lens = [min(l, X.shape[1]) for l in lens]\n",
    "lens = np.array(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9506"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]-249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training, val, and test sets. \n",
    "trainsets = []\n",
    "inds = np.arange(len(X))\n",
    "\n",
    "# Day of the data we used for testing\n",
    "test_day_inds = copy.deepcopy(inds[-249:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9755, 300, 506)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore these\n",
    "off_limits = list(test_day_inds)\n",
    "\n",
    "test_inds_eligible= inds\n",
    "np.random.seed(1337)\n",
    "np.random.shuffle(test_inds_eligible)\n",
    "\n",
    "\n",
    "# Use 5% of the data for val, 5% test, 90% to train.\n",
    "# The final test day data isn't touched yet. \n",
    "for k in range(10): \n",
    "    te_inds = test_inds_eligible[k*(len(inds)//20): (k+1)*(len(inds)//20)]\n",
    "    te_inds = [i for i in te_inds if not i in off_limits] # for test set during training. \n",
    "    val_inds = test_inds_eligible[(k+2)*(len(inds)//20):(k+3)*(len(inds)//20)]\n",
    "    val_inds = [i for i in val_inds if not i in te_inds and not i in off_limits]\n",
    "    tr_inds = [i for i in inds if not i in list(te_inds) + list(val_inds)]    \n",
    "    tr_inds = [i for i in tr_inds if not i in off_limits]\n",
    "    trainsets.append((tr_inds, val_inds, te_inds))\n",
    "\n",
    "# Get out the ground truth text. \n",
    "gt_text = labels['txt_label'].values\n",
    "tokens\n",
    "\n",
    "# Choose which fold to evaluate. \n",
    "if not args['eval_set'] is None: \n",
    "    eval_set = args['eval_set']\n",
    "    trainsets = trainsets[eval_set:eval_set+1]\n",
    "    \n",
    "\n",
    "from data_loading_utilities.torch_ctc_loaders import CTCDataset, Jitter, Blackout, AdditiveNoise, LevelChannelNoise, ScaleAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 0.9551356218945801 1.0713824626558794\n"
     ]
    }
   ],
   "source": [
    "# Now lets set up dataloaders and augmentations.\n",
    "b1args = {'winstart': 0, 'winend': 7.5,  'additive_noise_level': 0.0027354917297051813, \n",
    "        'scale_augment_low': 0.9551356218945801, 'scale_augment_high': 1.0713824626558794, \n",
    "        'blackout_len': 0.30682868940865543, 'blackout_prob': 0.04787032280216536,\n",
    "        'random_channel_noise_sigma': 0.028305685438945617\n",
    "        }\n",
    "#\n",
    "train_jitter = Jitter((-1, 8), (args['winstart'], args['winend']), jitter_amt=args['jitter_amt'], decimation=args['decimation'])\n",
    "test_jitter = Jitter((-1,8), (args['winstart'], args['winend']), jitter_amt=0.0, decimation=args['decimation'])\n",
    "train_jitter.winsize, test_jitter.winsize\n",
    "\n",
    "lens[:] = train_jitter.winsize\n",
    "\n",
    "blackout = Blackout(b1args['blackout_len'], args['blackout_prob'])\n",
    "noise = AdditiveNoise(b1args['additive_noise_level'])\n",
    "chan_noise = LevelChannelNoise(args['chan_noise'])\n",
    "scale = ScaleAugment(b1args['scale_augment_low'], b1args['scale_augment_high'])\n",
    "\n",
    "composed = transforms.Compose([\n",
    "    train_jitter,  blackout , noise, chan_noise, scale\n",
    "])\n",
    "\n",
    "test_augs = transforms.Compose([\n",
    "    test_jitter\n",
    "])\n",
    "# test_augs\n",
    "from train.ctc_aux_loss_trainer import train_loop\n",
    "from models.cnn_rnn_w_aux import AUXCnnRnnClassifier\n",
    "from data_loading_utilities.torch_ctc_loaders import CTCDataset_Wordct\n",
    "\n",
    "\n",
    "# The word targets actually are not used (weight set to zero), but they are here in case you'd find them useful!\n",
    "word_targets = []\n",
    "for l in labels['txt_label'].values:\n",
    "    word_targets.append(len(l.split(' ')))\n",
    "\n",
    "n_wordtarg = np.max(word_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to expect from training: \n",
    "\n",
    "Model will run for around 100 epochs. There is some variability in performance based on initializations. We interrupted training here for the sake of time \n",
    "\n",
    "We are using a 3-gram LM and small beam search here - as a result accuracy is LOWER during training, than when you test on the final set\n",
    "\n",
    "The WER should end around 35-42% prior to testing on the realtime blocks\n",
    "\n",
    "Using the 5-gram LM and larger beam search will help get the WER much lower when we evaluate on the realtime blocks, below 30% without the blocking.\n",
    "\n",
    "Note: The saved output here was interrupted for the sake of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sizes tr/val/test 8555 450 450\n",
      "num samples 8555\n",
      "Xtr (8555, 300, 506) Y_tr (8555, 46)\n",
      "(9755, 300, 506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/server/path/repos/pub_code/text/models/cnn_rnn_w_aux.py:43: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  lens = lens//self.ks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net wer 1.0\n",
      "net cer 1.0\n",
      "greedy per 0.7730677566431435\n",
      "beam per 0.7870946146226481\n",
      "gt are you kidding me\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt what if i did\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt for the time being\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt that would explain it\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt get out of there\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt i have a wonderful new play\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt then how come it worked loose\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt we all have problems\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt would you know the room\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt no need to apologize\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt that i would find the one\n",
      "t  wer: 1.00 cer: 1.00\n",
      "epoch 0 tr loss: 0.050 te_loss: 0.047\n",
      "epoch 1 tr loss: 0.036 te_loss: 0.034\n",
      "epoch 2 tr loss: 0.029 te_loss: 0.031\n",
      "net wer 0.7996031746031754\n",
      "net cer 0.6714796038535238\n",
      "greedy per 0.4758657507300975\n",
      "beam per 0.5709653929805514\n",
      "gt feels good to be back\n",
      "t will you be wer: 0.80 cer: 0.71\n",
      "gt you go to school together\n",
      "t you go to you wer: 0.40 cer: 0.56\n",
      "gt i wish we could hear them\n",
      "t i know wer: 0.83 cer: 0.88\n",
      "gt nobody else had it\n",
      "t i know it wer: 0.75 cer: 0.78\n",
      "gt when did it all start\n",
      "t i know wer: 1.00 cer: 0.90\n",
      "gt our man is sick come right away\n",
      "t did he will wer: 1.00 cer: 0.84\n",
      "gt nothing nothing at all\n",
      "t listen what wer: 1.00 cer: 0.77\n",
      "gt what are you wearing\n",
      "t like you wer: 0.75 cer: 0.75\n",
      "gt anyone asked for me\n",
      "t what are we wer: 1.00 cer: 0.74\n",
      "gt not with all your money\n",
      "t will you wer: 1.00 cer: 0.65\n",
      "gt everybody knew him he got around\n",
      "t how do we want wer: 1.00 cer: 0.75\n",
      "epoch 3 tr loss: 0.027 te_loss: 0.029\n",
      "epoch 4 tr loss: 0.025 te_loss: 0.026\n",
      "epoch 5 tr loss: 0.023 te_loss: 0.026\n",
      "net wer 0.682992063492064\n",
      "net cer 0.5631902895203267\n",
      "greedy per 0.4081303021104124\n",
      "beam per 0.48519563616436007\n",
      "gt they look like good kids\n",
      "t i get it wer: 1.00 cer: 0.79\n",
      "gt well what would you call him\n",
      "t would you call him wer: 0.33 cer: 0.36\n",
      "gt but you did it it was you\n",
      "t did you do with you wer: 0.71 cer: 0.48\n",
      "gt tell him to look through it\n",
      "t what do they know wer: 1.00 cer: 0.74\n",
      "gt i think i talked to him\n",
      "t i think i talk to him wer: 0.17 cer: 0.09\n",
      "gt what kind of test\n",
      "t but i did wer: 1.00 cer: 0.76\n",
      "gt this has to happen fast\n",
      "t did you have wer: 1.00 cer: 0.74\n",
      "gt i thought you stopped going\n",
      "t i thought you say wer: 0.40 cer: 0.44\n",
      "gt never read a book in my whole life\n",
      "t then what is your life wer: 0.88 cer: 0.62\n",
      "gt ask them to sit down\n",
      "t i love you see now wer: 1.00 cer: 0.65\n",
      "gt nobody ever wants me\n",
      "t no it was me wer: 0.75 cer: 0.50\n",
      "epoch 6 tr loss: 0.022 te_loss: 0.024\n",
      "epoch 7 tr loss: 0.021 te_loss: 0.024\n",
      "epoch 8 tr loss: 0.020 te_loss: 0.023\n",
      "net wer 0.6239444444444445\n",
      "net cer 0.508048976559648\n",
      "greedy per 0.38366046069979276\n",
      "beam per 0.44264874898269907\n",
      "gt you said that last time\n",
      "t you say that wer: 0.60 cer: 0.52\n",
      "gt well he asked me for it\n",
      "t what are we for it wer: 0.67 cer: 0.43\n",
      "gt your friend saw them\n",
      "t you want them wer: 0.75 cer: 0.50\n",
      "gt we can look into that\n",
      "t what is that wer: 0.80 cer: 0.57\n",
      "gt should i hit it\n",
      "t do it wer: 0.75 cer: 0.73\n",
      "gt i want to try\n",
      "t i want to do wer: 0.25 cer: 0.23\n",
      "gt what makes you sound so certain\n",
      "t what do you know wer: 0.67 cer: 0.61\n",
      "gt not so good you sure you have to\n",
      "t did you do you have to wer: 0.50 cer: 0.44\n",
      "gt are you and mom home tonight\n",
      "t are you wer: 0.67 cer: 0.75\n",
      "gt no i want to draw\n",
      "t no i want to talk wer: 0.20 cer: 0.24\n",
      "gt maybe you want to play\n",
      "t you want to do wer: 0.40 cer: 0.45\n",
      "epoch 9 tr loss: 0.019 te_loss: 0.023\n",
      "epoch 10 tr loss: 0.019 te_loss: 0.022\n",
      "epoch 11 tr loss: 0.018 te_loss: 0.022\n",
      "net wer 0.566515873015873\n",
      "net cer 0.4587670165570082\n",
      "greedy per 0.3668425615013354\n",
      "beam per 0.3973239595173136\n",
      "gt when did you write it\n",
      "t what did you find it wer: 0.40 cer: 0.29\n",
      "gt just tell me where she is\n",
      "t just tell me why wer: 0.50 cer: 0.40\n",
      "gt you want me to move\n",
      "t you want me to me wer: 0.20 cer: 0.11\n",
      "gt you believe in it\n",
      "t you believe in it wer: 0.00 cer: 0.00\n",
      "gt four or five days ago why\n",
      "t i did wer: 1.00 cer: 0.88\n",
      "gt are you and mom home tonight\n",
      "t you might wer: 0.83 cer: 0.68\n",
      "gt you mean now or before\n",
      "t you mean now or before wer: 0.00 cer: 0.00\n",
      "gt take it easy pop\n",
      "t it is wer: 0.75 cer: 0.75\n",
      "gt i saw you today\n",
      "t talk to you wer: 0.75 cer: 0.73\n",
      "gt can i call you at the office\n",
      "t can i get you wer: 0.57 cer: 0.64\n",
      "gt what is that to you\n",
      "t what is that to you wer: 0.00 cer: 0.00\n",
      "epoch 12 tr loss: 0.017 te_loss: 0.022\n",
      "epoch 13 tr loss: 0.016 te_loss: 0.022\n",
      "epoch 14 tr loss: 0.015 te_loss: 0.021\n",
      "net wer 0.5285211640211642\n",
      "net cer 0.4196376323387991\n",
      "greedy per 0.3440482273837157\n",
      "beam per 0.368232434620108\n",
      "gt he needs a doctor\n",
      "t is it wer: 1.00 cer: 0.82\n",
      "gt no need to apologize\n",
      "t you find it wer: 1.00 cer: 0.80\n",
      "gt so lay it out my brother\n",
      "t so like it out my father wer: 0.33 cer: 0.25\n",
      "gt four or five days ago why\n",
      "t may i wer: 1.00 cer: 0.88\n",
      "gt then i love it\n",
      "t leave it wer: 0.75 cer: 0.57\n",
      "gt you want it in writing\n",
      "t you want it is wer: 0.40 cer: 0.41\n",
      "gt why are you on to him\n",
      "t what are you got to him wer: 0.33 cer: 0.19\n",
      "gt maybe you want to play\n",
      "t maybe you want to play wer: 0.00 cer: 0.00\n",
      "gt you know the answer to that\n",
      "t you know the do that wer: 0.33 cer: 0.30\n",
      "gt have you got a minute\n",
      "t have you got a minute wer: 0.00 cer: 0.00\n",
      "gt did you have a nice time\n",
      "t did you have a nice time wer: 0.00 cer: 0.00\n",
      "epoch 15 tr loss: 0.015 te_loss: 0.021\n",
      "epoch 16 tr loss: 0.014 te_loss: 0.021\n",
      "epoch 17 tr loss: 0.013 te_loss: 0.021\n",
      "net wer 0.4754285714285714\n",
      "net cer 0.3684751163292351\n",
      "greedy per 0.3323677501633651\n",
      "beam per 0.32943187303814764\n",
      "gt well then check back tomorrow\n",
      "t well that not fine wer: 0.80 cer: 0.66\n",
      "gt they really made her\n",
      "t she really for her wer: 0.50 cer: 0.30\n",
      "gt are you seeing anyone i mean seriously\n",
      "t i see it be different wer: 1.00 cer: 0.76\n",
      "gt what gave me away\n",
      "t what gave me away wer: 0.00 cer: 0.00\n",
      "gt you think a woman did that\n",
      "t you think i did that wer: 0.33 cer: 0.27\n",
      "gt i had no idea you were even here\n",
      "t i have to ask you in here wer: 0.62 cer: 0.47\n",
      "gt if i understand what you meant by that\n",
      "t i just say what you been for that wer: 0.62 cer: 0.42\n",
      "gt what would do that\n",
      "t what would do that wer: 0.00 cer: 0.00\n",
      "gt you got a nice tan though\n",
      "t you can not wer: 0.83 cer: 0.64\n",
      "gt what was it like\n",
      "t what was it like wer: 0.00 cer: 0.00\n",
      "gt what is that to you\n",
      "t no what is that to you wer: 0.20 cer: 0.16\n",
      "epoch 18 tr loss: 0.013 te_loss: 0.021\n",
      "epoch 19 tr loss: 0.012 te_loss: 0.021\n",
      "epoch 20 tr loss: 0.011 te_loss: 0.021\n",
      "net wer 0.4567222222222221\n",
      "net cer 0.35382210612377085\n",
      "greedy per 0.32691006416202173\n",
      "beam per 0.31630644413178627\n",
      "gt you have to have a favorite\n",
      "t you have to have a favor wer: 0.17 cer: 0.11\n",
      "gt this is not cool\n",
      "t this is not go wer: 0.25 cer: 0.19\n",
      "gt nice to see you\n",
      "t nice to see you wer: 0.00 cer: 0.00\n",
      "gt what happened that night\n",
      "t what happened that take wer: 0.25 cer: 0.21\n",
      "gt did it hurt a lot\n",
      "t this is all this wer: 1.00 cer: 0.76\n",
      "gt i just came out to say goodbye\n",
      "t i have to say goodbye wer: 0.43 cer: 0.37\n",
      "gt i will be good kids\n",
      "t i will be good wer: 0.20 cer: 0.26\n",
      "gt our man is sick come right away\n",
      "t how much of a way wer: 1.00 cer: 0.68\n",
      "gt and everything is nothing too\n",
      "t everything is nothing too wer: 0.20 cer: 0.14\n",
      "gt do you need help\n",
      "t do you need help wer: 0.00 cer: 0.00\n",
      "gt you told him you liked him\n",
      "t do you come you like some wer: 0.83 cer: 0.46\n",
      "epoch 21 tr loss: 0.011 te_loss: 0.021\n",
      "epoch 22 tr loss: 0.010 te_loss: 0.021\n",
      "epoch 23 tr loss: 0.010 te_loss: 0.021\n",
      "net wer 0.4448756613756611\n",
      "net cer 0.34076249402722775\n",
      "greedy per 0.3263701870750344\n",
      "beam per 0.3056238026163198\n",
      "gt get going on the trap door\n",
      "t it would on the right wer: 0.67 cer: 0.54\n",
      "gt like maybe a big part\n",
      "t well for a way for wer: 0.80 cer: 0.71\n",
      "gt do you feel that way\n",
      "t do you feel that way wer: 0.00 cer: 0.00\n",
      "gt what are you dialing\n",
      "t what are you saying wer: 0.25 cer: 0.15\n",
      "gt have you got a minute\n",
      "t have you got a minute wer: 0.00 cer: 0.00\n",
      "gt this time with me\n",
      "t this time with me wer: 0.00 cer: 0.00\n",
      "gt this happened to you\n",
      "t we have to you wer: 0.50 cer: 0.45\n",
      "gt give me my book\n",
      "t let me my book wer: 0.25 cer: 0.27\n",
      "gt nobody else had it\n",
      "t to be in on it wer: 1.00 cer: 0.67\n",
      "gt would you really how sweet\n",
      "t would you will you do wer: 0.60 cer: 0.42\n",
      "gt this is not cool\n",
      "t this is not go wer: 0.25 cer: 0.19\n",
      "epoch 24 tr loss: 0.010 te_loss: 0.022\n",
      "epoch 25 tr loss: 0.009 te_loss: 0.023\n",
      "epoch 26 tr loss: 0.009 te_loss: 0.022\n",
      "net wer 0.42190211640211606\n",
      "net cer 0.3167965798963359\n",
      "greedy per 0.31896638134076977\n",
      "beam per 0.28447549149670875\n",
      "gt you take this much time over everything\n",
      "t you said this must stop over everything wer: 0.43 cer: 0.23\n",
      "gt when did you write it\n",
      "t what did you write it wer: 0.20 cer: 0.10\n",
      "gt well you got it now\n",
      "t well you got it now wer: 0.00 cer: 0.00\n",
      "gt is it my turn\n",
      "t is it i saw you wer: 0.75 cer: 0.69\n",
      "gt can we have a white one\n",
      "t i mean have want one wer: 0.67 cer: 0.48\n",
      "gt is that a fact\n",
      "t is that a fact wer: 0.00 cer: 0.00\n",
      "gt yes a little white house\n",
      "t that was out wer: 1.00 cer: 0.71\n",
      "gt find anything we missed\n",
      "t but we made wer: 0.75 cer: 0.70\n",
      "gt keep them out of my way\n",
      "t keep them out of my way wer: 0.00 cer: 0.00\n",
      "gt and now he will\n",
      "t i never get will wer: 0.75 cer: 0.60\n",
      "gt did he ever get rough with you\n",
      "t did i ever get rough with you wer: 0.14 cer: 0.07\n",
      "epoch 27 tr loss: 0.008 te_loss: 0.022\n",
      "epoch 28 tr loss: 0.008 te_loss: 0.022\n",
      "epoch 29 tr loss: 0.008 te_loss: 0.023\n",
      "net wer 0.4144074074074069\n",
      "net cer 0.31776727994015125\n",
      "greedy per 0.3192005411595825\n",
      "beam per 0.28618629585593536\n",
      "gt tell me how all this works\n",
      "t let me have it this worse wer: 0.67 cer: 0.42\n",
      "gt have a good day to\n",
      "t i heard you wer: 1.00 cer: 0.72\n",
      "gt is that a fact\n",
      "t is that a fact wer: 0.00 cer: 0.00\n",
      "gt how did you hurt your hand\n",
      "t how did you get your hand wer: 0.17 cer: 0.12\n",
      "gt am i free to go\n",
      "t it was me to go wer: 0.60 cer: 0.47\n",
      "gt i know what i see\n",
      "t i know what i do wer: 0.20 cer: 0.18\n",
      "gt when did it all start\n",
      "t he got ten wer: 1.00 cer: 0.71\n",
      "gt i had no idea you were even here\n",
      "t i had no and you come here wer: 0.38 cer: 0.38\n",
      "gt i hate the weird ones\n",
      "t i guess we met wer: 0.80 cer: 0.62\n",
      "gt anywhere i think we should just keep moving\n",
      "t i have to say wer: 0.88 cer: 0.81\n",
      "gt what does it say there\n",
      "t what does it say there wer: 0.00 cer: 0.00\n",
      "epoch 30 tr loss: 0.007 te_loss: 0.023\n",
      "epoch 31 tr loss: 0.007 te_loss: 0.023\n",
      "epoch 32 tr loss: 0.007 te_loss: 0.024\n",
      "net wer 0.4134682539682539\n",
      "net cer 0.31043143808584944\n",
      "greedy per 0.3251745873524425\n",
      "beam per 0.28057742955827625\n",
      "gt anything happen while i was away\n",
      "t well i was way wer: 0.67 cer: 0.62\n",
      "gt how did you hurt your hand\n",
      "t how did you hurt your eye wer: 0.17 cer: 0.15\n",
      "gt not for you clearly\n",
      "t not for you wer: 0.25 cer: 0.42\n",
      "gt this is not cool\n",
      "t this is not cool wer: 0.00 cer: 0.00\n",
      "gt with an old friend\n",
      "t will it only one wer: 1.00 cer: 0.61\n",
      "gt then you tell me what just happened\n",
      "t like you tell me what happened wer: 0.29 cer: 0.26\n",
      "gt i never forgot that\n",
      "t i never forgot that wer: 0.00 cer: 0.00\n",
      "gt keep them out of my way\n",
      "t keep them out of my way wer: 0.00 cer: 0.00\n",
      "gt get going on the trap door\n",
      "t could we on the right wer: 0.67 cer: 0.65\n",
      "gt just tell me where she is\n",
      "t just tell me what she then wer: 0.33 cer: 0.28\n",
      "gt i saw him with you\n",
      "t i saw him with you wer: 0.00 cer: 0.00\n",
      "epoch 33 tr loss: 0.007 te_loss: 0.023\n",
      "epoch 34 tr loss: 0.007 te_loss: 0.023\n",
      "epoch 35 tr loss: 0.006 te_loss: 0.023\n",
      "net wer 0.40931216931216935\n",
      "net cer 0.30749004135781327\n",
      "greedy per 0.31938860135941927\n",
      "beam per 0.28012640545876827\n",
      "gt i have some upstairs\n",
      "t i have someone please wer: 0.50 cer: 0.45\n",
      "gt i know what i see\n",
      "t i know what i did wer: 0.20 cer: 0.18\n",
      "gt have you got a minute\n",
      "t have you got a minute wer: 0.00 cer: 0.00\n",
      "gt is that what you want me to be\n",
      "t is that what you want me to me wer: 0.12 cer: 0.03\n",
      "gt what line is he in\n",
      "t what that is it wer: 0.60 cer: 0.44\n",
      "gt do you mind if i sit\n",
      "t do you mind if i said wer: 0.17 cer: 0.10\n",
      "gt thought you were hungry\n",
      "t thought you were agree wer: 0.25 cer: 0.22\n",
      "gt for the time being\n",
      "t for the night minute wer: 0.50 cer: 0.50\n",
      "gt somebody had to go\n",
      "t somebody had to go wer: 0.00 cer: 0.00\n",
      "gt while he was here you went about together\n",
      "t i was here you were about to get wer: 0.62 cer: 0.32\n",
      "gt i knew you would\n",
      "t why do you want wer: 0.75 cer: 0.62\n",
      "epoch 36 tr loss: 0.006 te_loss: 0.023\n",
      "epoch 37 tr loss: 0.006 te_loss: 0.024\n",
      "epoch 38 tr loss: 0.006 te_loss: 0.025\n",
      "net wer 0.40815873015872994\n",
      "net cer 0.3073838814724054\n",
      "greedy per 0.32168652819430477\n",
      "beam per 0.2769004550042096\n",
      "gt they were kidding you\n",
      "t think we get you wer: 0.75 cer: 0.57\n",
      "gt i was missing you\n",
      "t i want me to wer: 0.75 cer: 0.59\n",
      "gt you could do it\n",
      "t you could do it wer: 0.00 cer: 0.00\n",
      "gt does it please you to say such things\n",
      "t this is man to to just like that wer: 0.88 cer: 0.62\n",
      "gt there is one thing i need right away\n",
      "t there is one thing i did what way wer: 0.38 cer: 0.22\n",
      "gt nobody else had it\n",
      "t to be in on it wer: 1.00 cer: 0.67\n",
      "gt i told you what they said\n",
      "t i told you watch the set wer: 0.50 cer: 0.28\n",
      "gt sure that would be cool friends\n",
      "t that would be fine wer: 0.50 cer: 0.45\n",
      "gt this happened to you\n",
      "t this have to you wer: 0.25 cer: 0.25\n",
      "gt i sure know what that feels like\n",
      "t i know what that would take wer: 0.43 cer: 0.34\n",
      "gt you have to tell us now\n",
      "t you have to say now wer: 0.33 cer: 0.30\n",
      "epoch 39 tr loss: 0.006 te_loss: 0.024\n",
      "epoch 40 tr loss: 0.006 te_loss: 0.025\n",
      "epoch 41 tr loss: 0.006 te_loss: 0.024\n",
      "net wer 0.4042116402116402\n",
      "net cer 0.2978852518802955\n",
      "greedy per 0.3186880822350656\n",
      "beam per 0.27043618212155135\n",
      "gt your friends use the pool too\n",
      "t your friends to the will you wer: 0.50 cer: 0.28\n",
      "gt do you mind if i sit\n",
      "t do you mind if i sit wer: 0.00 cer: 0.00\n",
      "gt can we go somewhere and talk\n",
      "t can we go somewhere and talk wer: 0.00 cer: 0.00\n",
      "gt can anyone hear what we say\n",
      "t and how we say wer: 0.67 cer: 0.56\n",
      "gt why do you want to know my name\n",
      "t why do you want to know my name wer: 0.00 cer: 0.00\n",
      "gt you have to come\n",
      "t you have to come wer: 0.00 cer: 0.00\n",
      "gt just seen this all red house\n",
      "t this is that a way out wer: 1.00 cer: 0.64\n",
      "gt i wish she could forget him\n",
      "t i want you call him wer: 0.67 cer: 0.59\n",
      "gt what if i did\n",
      "t what if i did wer: 0.00 cer: 0.00\n",
      "gt i really want to go somewhere this summer\n",
      "t i really want to go home this ever wer: 0.25 cer: 0.24\n",
      "gt if i understand what you meant by that\n",
      "t am i understand what you meant by that wer: 0.12 cer: 0.05\n",
      "epoch 42 tr loss: 0.006 te_loss: 0.025\n",
      "epoch 43 tr loss: 0.006 te_loss: 0.025\n",
      "epoch 44 tr loss: 0.005 te_loss: 0.025\n",
      "net wer 0.3938624338624334\n",
      "net cer 0.2938813202134287\n",
      "greedy per 0.3138469473185867\n",
      "beam per 0.2648600170602921\n",
      "gt i thought i was doing you a favor\n",
      "t i thought i was doing you a favor wer: 0.00 cer: 0.00\n",
      "gt your leg must be getting better\n",
      "t you think must be getting better wer: 0.33 cer: 0.19\n",
      "gt can we have a white one\n",
      "t i have a watch one wer: 0.50 cer: 0.43\n",
      "gt thank me next time you see me\n",
      "t let me done time you leave me wer: 0.43 cer: 0.41\n",
      "gt i want to try\n",
      "t i want to say wer: 0.25 cer: 0.15\n",
      "gt i saw you today\n",
      "t i saw you today wer: 0.00 cer: 0.00\n",
      "gt you said it a million times\n",
      "t you said it in times wer: 0.33 cer: 0.26\n",
      "gt did you love him\n",
      "t did you love him wer: 0.00 cer: 0.00\n",
      "gt listen maybe i should go\n",
      "t just maybe i should go wer: 0.20 cer: 0.17\n",
      "gt do you study at all\n",
      "t do you stay at all wer: 0.20 cer: 0.11\n",
      "gt why did you hang up on me\n",
      "t what did you up on me wer: 0.29 cer: 0.28\n",
      "epoch 45 tr loss: 0.005 te_loss: 0.025\n",
      "epoch 46 tr loss: 0.005 te_loss: 0.025\n",
      "epoch 47 tr loss: 0.005 te_loss: 0.025\n",
      "net wer 0.39652380952380945\n",
      "net cer 0.2957723186704017\n",
      "greedy per 0.3114630522342545\n",
      "beam per 0.26737522048836637\n",
      "gt you in this with me\n",
      "t you can this with me wer: 0.20 cer: 0.11\n",
      "gt so how come you got into all this\n",
      "t no i hope you knew this wer: 0.75 cer: 0.52\n",
      "gt this is not cool\n",
      "t this is not go wer: 0.25 cer: 0.19\n",
      "gt well actually she looks better than that\n",
      "t what did this more than that wer: 0.71 cer: 0.55\n",
      "gt look under the body\n",
      "t you answer the money wer: 0.75 cer: 0.47\n",
      "gt are you trying to be funny\n",
      "t are you going to be funny wer: 0.17 cer: 0.12\n",
      "gt a business meeting i told you that\n",
      "t making me i know you that wer: 0.57 cer: 0.50\n",
      "gt do you mind if i sit\n",
      "t do you mind if i did wer: 0.17 cer: 0.10\n",
      "gt are you asking me out\n",
      "t i want you out wer: 0.80 cer: 0.67\n",
      "gt what kind of person\n",
      "t we had a minute wer: 1.00 cer: 0.74\n",
      "gt was quite excited when he came in\n",
      "t what were another way he came in wer: 0.57 cer: 0.45\n",
      "epoch 48 tr loss: 0.005 te_loss: 0.026\n",
      "epoch 49 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 50 tr loss: 0.005 te_loss: 0.026\n",
      "net wer 0.3797804232804232\n",
      "net cer 0.280112590612278\n",
      "greedy per 0.3146337935639015\n",
      "beam per 0.25171490846423183\n",
      "gt i think i scared it\n",
      "t i think i can wer: 0.40 cer: 0.37\n",
      "gt got run over by a car\n",
      "t what would you be a car wer: 0.67 cer: 0.57\n",
      "gt tell me what he said\n",
      "t now we what he said wer: 0.40 cer: 0.25\n",
      "gt did you hear what i said\n",
      "t no you hear what i said wer: 0.17 cer: 0.12\n",
      "gt never heard of him thanks anyway\n",
      "t you will of him that anyway wer: 0.50 cer: 0.41\n",
      "gt nice to see you\n",
      "t nice to see you wer: 0.00 cer: 0.00\n",
      "gt can you play it\n",
      "t can you play it wer: 0.00 cer: 0.00\n",
      "gt give me that again\n",
      "t give me that again wer: 0.00 cer: 0.00\n",
      "gt that will never happen\n",
      "t that will never happen wer: 0.00 cer: 0.00\n",
      "gt no one of us is enough\n",
      "t no one of them wer: 0.50 cer: 0.50\n",
      "gt have a good day to\n",
      "t am i could you do wer: 1.00 cer: 0.61\n",
      "epoch 51 tr loss: 0.005 te_loss: 0.026\n",
      "epoch 52 tr loss: 0.005 te_loss: 0.026\n",
      "epoch 53 tr loss: 0.005 te_loss: 0.025\n",
      "net wer 0.38270634920634894\n",
      "net cer 0.2833698937376568\n",
      "greedy per 0.30876665475897097\n",
      "beam per 0.25784810691826987\n",
      "gt well he asked me for it\n",
      "t well for it wer: 0.50 cer: 0.52\n",
      "gt thanks for the great time\n",
      "t thanks for the good time wer: 0.20 cer: 0.16\n",
      "gt did he ever get rough with you\n",
      "t did it ever get rough with you wer: 0.14 cer: 0.07\n",
      "gt yes i lost my remote control\n",
      "t yes i thought you been couple wer: 0.67 cer: 0.64\n",
      "gt i told him that you all is sick\n",
      "t i told him that you wear it wer: 0.38 cer: 0.29\n",
      "gt what are you two talking about\n",
      "t what are you two talking about wer: 0.00 cer: 0.00\n",
      "gt how can i go\n",
      "t how can i go wer: 0.00 cer: 0.00\n",
      "gt i knew you would\n",
      "t i do you want wer: 0.50 cer: 0.50\n",
      "gt i hate the weird ones\n",
      "t i guess we met wer: 0.80 cer: 0.62\n",
      "gt well then sit down\n",
      "t well that sit down wer: 0.25 cer: 0.11\n",
      "gt for half a million\n",
      "t for half a minute wer: 0.25 cer: 0.28\n",
      "epoch 54 tr loss: 0.005 te_loss: 0.026\n",
      "epoch 55 tr loss: 0.005 te_loss: 0.026\n",
      "epoch 56 tr loss: 0.005 te_loss: 0.026\n",
      "net wer 0.38480158730158714\n",
      "net cer 0.2779694102189599\n",
      "greedy per 0.3110107909645727\n",
      "beam per 0.2559957977395469\n",
      "gt for the time being\n",
      "t for the time mean wer: 0.25 cer: 0.17\n",
      "gt believe you about what\n",
      "t pay for one wer: 1.00 cer: 0.82\n",
      "gt they could see that i meant it\n",
      "t did you do that first it wer: 0.71 cer: 0.53\n",
      "gt i was doing my job\n",
      "t i want to lie down wer: 0.80 cer: 0.67\n",
      "gt did you make any change in this room\n",
      "t did you make sure thing wer: 0.62 cer: 0.50\n",
      "gt well then sit down\n",
      "t well that sit down wer: 0.25 cer: 0.11\n",
      "gt so how come you got into all this\n",
      "t no you get all this wer: 0.62 cer: 0.48\n",
      "gt no one of us is enough\n",
      "t no one of an enough wer: 0.33 cer: 0.23\n",
      "gt there is one thing i need right away\n",
      "t like it right think i need one way wer: 0.75 cer: 0.47\n",
      "gt you told him you liked him\n",
      "t do you come you liked him wer: 0.50 cer: 0.31\n",
      "gt you can see for yourself\n",
      "t you on some more yourself wer: 0.60 cer: 0.25\n",
      "epoch 57 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 58 tr loss: 0.005 te_loss: 0.026\n",
      "epoch 59 tr loss: 0.005 te_loss: 0.026\n",
      "net wer 0.396253968253968\n",
      "net cer 0.2957048604749981\n",
      "greedy per 0.3211643037529384\n",
      "beam per 0.2665487887960371\n",
      "gt what kind of person\n",
      "t what are you wer: 0.75 cer: 0.63\n",
      "gt are you guys hurt\n",
      "t i knew it could wer: 1.00 cer: 0.88\n",
      "gt i have to take a shower\n",
      "t i have to take a shower wer: 0.00 cer: 0.00\n",
      "gt you know the answer to that\n",
      "t you know the answer to that wer: 0.00 cer: 0.00\n",
      "gt you know i saw you on the street\n",
      "t you know i saw you on the ring wer: 0.12 cer: 0.16\n",
      "gt did you call someone you knew\n",
      "t did you get one to you wer: 0.67 cer: 0.48\n",
      "gt have fun at work\n",
      "t fine a week wer: 1.00 cer: 0.62\n",
      "gt and what would this be sweet and low\n",
      "t i would this be we know wer: 0.62 cer: 0.42\n",
      "gt you never did answer my question\n",
      "t you never say in my question wer: 0.33 cer: 0.25\n",
      "gt anyway come already to the point\n",
      "t anyway come up to the man wer: 0.33 cer: 0.34\n",
      "gt i saw the show\n",
      "t i saw the show wer: 0.00 cer: 0.00\n",
      "epoch 60 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 61 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 62 tr loss: 0.005 te_loss: 0.027\n",
      "net wer 0.37941005291005275\n",
      "net cer 0.2771660013319375\n",
      "greedy per 0.31007182669603706\n",
      "beam per 0.25370663651501124\n",
      "gt am i free to go\n",
      "t if i mean to go wer: 0.40 cer: 0.40\n",
      "gt buy you a drink\n",
      "t but you a drink wer: 0.25 cer: 0.07\n",
      "gt are you trying to be funny\n",
      "t are you going to be father wer: 0.33 cer: 0.31\n",
      "gt why would i tell mom\n",
      "t are you wer: 1.00 cer: 0.85\n",
      "gt was quite excited when he came in\n",
      "t what way would he came in wer: 0.57 cer: 0.55\n",
      "gt cute young thing too\n",
      "t you like the door wer: 1.00 cer: 0.70\n",
      "gt but we made a deal\n",
      "t but we made a deal wer: 0.00 cer: 0.00\n",
      "gt have a good day to\n",
      "t if i could wer: 1.00 cer: 0.78\n",
      "gt you know who i am august\n",
      "t you know who i am wer: 0.17 cer: 0.29\n",
      "gt why are you stopping\n",
      "t why are you coming wer: 0.25 cer: 0.20\n",
      "gt come home with me\n",
      "t come home with me wer: 0.00 cer: 0.00\n",
      "epoch 63 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 64 tr loss: 0.005 te_loss: 0.026\n",
      "epoch 65 tr loss: 0.005 te_loss: 0.027\n",
      "net wer 0.3844470899470897\n",
      "net cer 0.2808628103166341\n",
      "greedy per 0.30868010304088833\n",
      "beam per 0.25693412012892186\n",
      "gt can we have a white one\n",
      "t i might have one one wer: 0.67 cer: 0.57\n",
      "gt give me the book\n",
      "t give me the book wer: 0.00 cer: 0.00\n",
      "gt we have never been friends\n",
      "t we never my friend wer: 0.60 cer: 0.38\n",
      "gt cute young thing too\n",
      "t you think the too wer: 0.75 cer: 0.55\n",
      "gt i should maybe talk to you\n",
      "t i should maybe talk to you wer: 0.00 cer: 0.00\n",
      "gt but i understand what you mean\n",
      "t but i understand what you do wer: 0.17 cer: 0.13\n",
      "gt what color hair will she have\n",
      "t i was it wer: 1.00 cer: 0.83\n",
      "gt then i love it\n",
      "t then a living wer: 0.75 cer: 0.43\n",
      "gt a group of us are going out tonight\n",
      "t are you am as a going out tonight wer: 0.62 cer: 0.29\n",
      "gt believe you about what\n",
      "t play for what wer: 0.75 cer: 0.59\n",
      "gt what is she doing here\n",
      "t what is she doing here wer: 0.00 cer: 0.00\n",
      "epoch 66 tr loss: 0.005 te_loss: 0.026\n",
      "epoch 67 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 68 tr loss: 0.005 te_loss: 0.027\n",
      "net wer 0.3845687830687828\n",
      "net cer 0.2798475134306646\n",
      "greedy per 0.3080074678721997\n",
      "beam per 0.25443573782450357\n",
      "gt there are only three or four left\n",
      "t there i call you or four left wer: 0.43 cer: 0.33\n",
      "gt are you kidding me\n",
      "t are you hit me wer: 0.25 cer: 0.33\n",
      "gt somebody had to go\n",
      "t somebody had to go wer: 0.00 cer: 0.00\n",
      "gt we had a crazy night\n",
      "t where are a pretty nine wer: 0.80 cer: 0.65\n",
      "gt you know how to write one of these\n",
      "t you know how to what one of these wer: 0.12 cer: 0.09\n",
      "gt why are you on to him\n",
      "t why are you on to him wer: 0.00 cer: 0.00\n",
      "gt hey i heard you went to summer school\n",
      "t how do you want to see you wer: 0.75 cer: 0.51\n",
      "gt there is nothing i can give\n",
      "t there is night i get him wer: 0.50 cer: 0.41\n",
      "gt i want the rest of the pictures\n",
      "t i want the rest of the pictures wer: 0.00 cer: 0.00\n",
      "gt do you feel that way\n",
      "t do you feel that way wer: 0.00 cer: 0.00\n",
      "gt they really made her\n",
      "t they believe back her wer: 0.50 cer: 0.45\n",
      "epoch 69 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 70 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 71 tr loss: 0.005 te_loss: 0.027\n",
      "net wer 0.38443650793650763\n",
      "net cer 0.27996965703346394\n",
      "greedy per 0.3132641212390803\n",
      "beam per 0.25497181753767234\n",
      "gt you have to do it\n",
      "t you have to do it wer: 0.00 cer: 0.00\n",
      "gt what do you believe in then\n",
      "t what do you like it that wer: 0.50 cer: 0.26\n",
      "gt i dropped her at home\n",
      "t i try to get home wer: 0.60 cer: 0.48\n",
      "gt they come with the house\n",
      "t they come with the house wer: 0.00 cer: 0.00\n",
      "gt no i want to draw\n",
      "t no i want to try wer: 0.20 cer: 0.18\n",
      "gt hope you find him i love that dog\n",
      "t you mind if i love the door wer: 0.62 cer: 0.36\n",
      "gt anyone asked for me\n",
      "t anyone asked for me wer: 0.00 cer: 0.00\n",
      "gt what is he doing in here\n",
      "t what did you hear wer: 0.83 cer: 0.58\n",
      "gt i know he left you behind\n",
      "t i know he left you man wer: 0.17 cer: 0.20\n",
      "gt what are you dialing\n",
      "t what are you saying wer: 0.25 cer: 0.15\n",
      "gt i sure know what that feels like\n",
      "t you know what that will take wer: 0.57 cer: 0.34\n",
      "epoch 72 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 73 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 74 tr loss: 0.005 te_loss: 0.028\n",
      "net wer 0.3831349206349204\n",
      "net cer 0.2799888013647567\n",
      "greedy per 0.3113453475613178\n",
      "beam per 0.2533416264477291\n",
      "gt he called me chicken\n",
      "t i called me see wer: 0.50 cer: 0.40\n",
      "gt i wanted to see who you were\n",
      "t i want to see you work wer: 0.43 cer: 0.29\n",
      "gt do you feel that way\n",
      "t do you feel that way wer: 0.00 cer: 0.00\n",
      "gt so what are we supposed to do\n",
      "t so what this is it wer: 0.71 cer: 0.62\n",
      "gt she said lots of things\n",
      "t she say that of these wer: 0.60 cer: 0.43\n",
      "gt to the bathroom where do you think\n",
      "t all the bathroom where do you think wer: 0.14 cer: 0.09\n",
      "gt he made me laugh\n",
      "t he might be love wer: 0.75 cer: 0.56\n",
      "gt be better than me\n",
      "t we might that be wer: 1.00 cer: 0.53\n",
      "gt so you have information for me\n",
      "t so you have fun me for me wer: 0.33 cer: 0.33\n",
      "gt our man is sick come right away\n",
      "t how much is just come with a boy wer: 0.86 cer: 0.52\n",
      "gt get in the bathroom\n",
      "t get in the bathroom wer: 0.00 cer: 0.00\n",
      "epoch 75 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 76 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 77 tr loss: 0.005 te_loss: 0.028\n",
      "net wer 0.39273809523809505\n",
      "net cer 0.2820609452271012\n",
      "greedy per 0.31130849127371574\n",
      "beam per 0.2598335715753222\n",
      "gt but you did it it was you\n",
      "t did you see the one you wer: 0.71 cer: 0.52\n",
      "gt you think he could still be in town\n",
      "t well thank you tell me it down wer: 1.00 cer: 0.46\n",
      "gt i really want to go somewhere this summer\n",
      "t i really want to go out the number wer: 0.38 cer: 0.29\n",
      "gt this has to happen fast\n",
      "t can you ever fun wer: 1.00 cer: 0.74\n",
      "gt and now he will\n",
      "t no it will wer: 0.75 cer: 0.47\n",
      "gt just saying goodbye to everyone before you leave\n",
      "t just say goodbye to everyone too wer: 0.50 cer: 0.35\n",
      "gt is it my turn\n",
      "t is it my dear wer: 0.25 cer: 0.31\n",
      "gt then you remember more than i do\n",
      "t then you before that i do wer: 0.43 cer: 0.28\n",
      "gt go back to your own time\n",
      "t go back to all time wer: 0.33 cer: 0.33\n",
      "gt already working on it\n",
      "t we were in it wer: 0.75 cer: 0.57\n",
      "gt no need to apologize\n",
      "t did you find it wer: 1.00 cer: 0.80\n",
      "epoch 78 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 79 tr loss: 0.005 te_loss: 0.027\n",
      "epoch 80 tr loss: 0.005 te_loss: 0.028\n",
      "net wer 0.39226455026455004\n",
      "net cer 0.28773221538487265\n",
      "greedy per 0.3104141710123056\n",
      "beam per 0.2627396221663438\n",
      "gt let me know when you do all right\n",
      "t let me know when you right wer: 0.25 cer: 0.21\n",
      "gt i run him off\n",
      "t i want him wer: 0.50 cer: 0.54\n",
      "gt we can cut back at the end\n",
      "t we can get back on the us wer: 0.43 cer: 0.27\n",
      "gt how long will you give me\n",
      "t how long will you give me wer: 0.00 cer: 0.00\n",
      "gt i never forgot that\n",
      "t i never forgot that wer: 0.00 cer: 0.00\n",
      "gt no you like the way i do things\n",
      "t do you like know i do think wer: 0.50 cer: 0.29\n",
      "gt what is she doing here\n",
      "t is she doing here wer: 0.20 cer: 0.23\n",
      "gt how do you know this place\n",
      "t how do you know this place wer: 0.00 cer: 0.00\n",
      "gt i thought i was doing you a favor\n",
      "t i thought i was doing you a favor wer: 0.00 cer: 0.00\n",
      "gt a business meeting i told you that\n",
      "t first me i know you that wer: 0.57 cer: 0.50\n",
      "gt thanks for the message\n",
      "t thanks for the message wer: 0.00 cer: 0.00\n",
      "epoch 81 tr loss: 0.005 te_loss: 0.028\n",
      "epoch 82 tr loss: 0.005 te_loss: 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "# Split data into train/val/test sets. \n",
    "for train, val, test in trainsets:\n",
    "    # Train test split, plus load into dataset\n",
    "    # Limit data to be the same for the k oflds\n",
    "    val = val[:450]\n",
    "    test = test[:450]\n",
    "    print('Sample sizes tr/val/test', len(train), len(val), len(test))\n",
    "    train_amt = int(len(train)*args['train_amt'])\n",
    "    print('num samples', train_amt)\n",
    "    wandb.log({'num_samples':train_amt})\n",
    "    train = train[:train_amt]\n",
    "    lens = np.array(list(lens))\n",
    "\n",
    "    word_targets = np.array(word_targets)\n",
    "\n",
    "    X_tr, X_val, X_te = X[train], X[val], X[test]\n",
    "    Y_tr, Y_val, Y_te = Y[train], Y[val], Y[test]\n",
    "    print('Xtr', X_tr.shape, 'Y_tr', Y_tr.shape)\n",
    "    lens_tr, lens_val, lens_te = lens[train], lens[val], lens[test]\n",
    "    inds_tr, inds_val, inds_te = np.array(train), np.array(val), np.array(test) # for loading text labels.\n",
    "    outlens_tr, outlens_val, outlens_te = outlens[train], outlens[val], outlens[test]\n",
    "    word_targs_tr, word_targs_val, word_targs_te = word_targets[train], word_targets[val], word_targets[test]\n",
    "    \n",
    "    train_dset = CTCDataset_Wordct(X_tr, Y_tr, lens_tr, outlens_tr, inds_tr, word_targs_tr, transform=composed)\n",
    "    val_dset = CTCDataset_Wordct(X_val, Y_val, lens_val, outlens_val, inds_val, word_targs_val, transform=test_augs)\n",
    "    test_dset = CTCDataset_Wordct(X_te, Y_te, lens_te, outlens_te, inds_te, word_targs_te, transform=test_augs)\n",
    "    \n",
    "    # TODO: Add transforms from torchaudio.transforms\n",
    "    train_loader = DataLoader(train_dset, batch_size=args['bs'], shuffle=True) \n",
    "    test_loader = DataLoader(test_dset, batch_size=min(X_te.shape[0], args['bs']), shuffle=False)\n",
    "    val_loader = DataLoader(val_dset, batch_size=min(args['bs'], X_val.shape[0]), shuffle=False)\n",
    "    \n",
    "    tdi = test_day_inds\n",
    "    print(X.shape)\n",
    "    X_final, Y_final, lens_final, outlens_final = X[tdi], Y[tdi], lens[tdi], outlens[tdi]\n",
    "    inds_final = np.array(tdi)\n",
    "    word_targs_final = word_targets[tdi]\n",
    "    final_dset = CTCDataset_Wordct(X_final, Y_final, lens_final, outlens_final, inds_final, word_targs_final, transform=test_augs)\n",
    "    \n",
    "    break\n",
    "\n",
    "\n",
    "# Now we load in the final realtime test blocks. \n",
    "final_loader = DataLoader(final_dset, batch_size=args['bs'], shuffle=False)\n",
    "    \n",
    "# Initialize the model. \n",
    "device='cuda'\n",
    "if not args['feedforward']:\n",
    "    if not args['pretrained'] is None: \n",
    "        n_targ = args['ndense']\n",
    "    else: \n",
    "        n_targ=len((enc_final))\n",
    "\n",
    "\n",
    "    if args['model_type'] == 'cnnrnn':\n",
    "        model = AUXCnnRnnClassifier(rnn_dim=args['hidden_dim'], KS=args['ks'], \n",
    "                                         num_layers=args['num_layers'],\n",
    "                                         dropout=args['dropout'], n_targ=n_targ,\n",
    "                                  bidirectional=True, in_channels=X_tr.shape[-1], nword_targ=n_wordtarg+1)\n",
    "\n",
    "wandb.log({\n",
    "    'n_targ':len((enc_final)),\n",
    "    'in_channels':X_tr.shape[-1]\n",
    "})\n",
    "\n",
    "if args['weight_decay'] is None:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "else: \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "model = model.to(device)\n",
    "\n",
    "# Now we run the train loop. It will train up the model. \n",
    "# Every three epochs, it will check in on the model perfomrance and see how it's doing \n",
    "# Training takes about 1.5-2 hours on an NVIDIA TITAN V GPU\n",
    "model =train_loop(model, train_loader,\n",
    "                val_loader, \n",
    "                  optimizer,\n",
    "                device, gt_text, greedy, beam_search_decoder, tokens, start_eval=0, \n",
    "                 wandb_log=True, checkpoint_dir=args['checkpoint_dir'], printall=args['printall'],\n",
    "                 wordloss_weight=args['word_ct_weight'], clipamt =args['clipamt'], \n",
    "                 max_epochs=1000)\n",
    "\n",
    "# Currently we only use one fold for model dev. \n",
    "\n",
    "from train.eval_script import test_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets run on the realtime test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token lm False\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define a new beam size, we found taht using a larger beam size\n",
    "# is quite helpful here and will lower the word error rate. \n",
    "beam_search_decoder_final = ctc_decoder(\n",
    "    lexicon = join(curdir, 'for_ctc/lexicon_phrases_1k.txt'),\n",
    "    tokens = join(curdir, 'for_ctc/tokens_phrases_1k.txt'),\n",
    "    lm = join(curdir, 'custom_lms/full_corpus_lm_5_abs_slm.binary'),\n",
    "    nbest=3,\n",
    "    beam_size=3000,\n",
    "    lm_weight=4.5,\n",
    "    word_score=args['WORD_SCORE'],\n",
    "    sil_token = '|', \n",
    "    unk_word = '<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/smetzger/repos/pub_code/text/models/cnn_rnn_w_aux.py:43: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  lens = lens//self.ks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net wer 0.28365366226812017\n",
      "net cer 0.21281817063458622\n",
      "greedy per 0.2853330700034549\n",
      "beam per 0.1941643652813536\n",
      "gt i thought everything was working out fine\n",
      "t i thought it was make of mine wer: 0.57 cer: 0.44\n",
      "gt come here i want to show you something\n",
      "t have i want to say something wer: 0.50 cer: 0.34\n",
      "gt what is it all about\n",
      "t what is she ask about wer: 0.40 cer: 0.25\n",
      "gt just look at all those people\n",
      "t she look at all those people wer: 0.17 cer: 0.14\n",
      "gt what did you do tonight\n",
      "t what do you do tonight wer: 0.20 cer: 0.09\n",
      "gt you were looking for us\n",
      "t you were looking for us wer: 0.00 cer: 0.00\n",
      "gt you got no idea what they were after\n",
      "t you got no idea what they were after wer: 0.00 cer: 0.00\n",
      "gt what kind of man are you\n",
      "t what kind of man are you wer: 0.00 cer: 0.00\n",
      "gt how did they find out\n",
      "t how did they find out wer: 0.00 cer: 0.00\n",
      "gt why would he pick this one\n",
      "t why would he pick this one wer: 0.00 cer: 0.00\n",
      "gt no one will believe us\n",
      "t no one will believe us wer: 0.00 cer: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Evaluate on totally UNSEEN sentences\n",
    "# We use the early stopping here as well\n",
    "# The averaged WER does not take into account the sentence lengths so will differ from reported wer. \n",
    "from train.eval_script import test_ensemble\n",
    "from train.sim_early_stop import test_ensemble_wpm\n",
    "# Now we load in the final realtime test blocks. \n",
    "# We need to set the batch size to be 1 to get wpms\n",
    "final_loader = DataLoader(final_dset, batch_size=1, shuffle=False)\n",
    "realtime_test_results = test_ensemble_wpm(model, final_loader, greedy, beam_search_decoder_final, gt_text, tokens, wandb_name = 'rt_', realtime_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results should be very close to what we got in realtime.\n",
    "wers = realtime_test_results[5]\n",
    "\n",
    "# Can see the ground truth\n",
    "gts = realtime_test_results[8]\n",
    "\n",
    "# Can check out the decoded text!\n",
    "decodes = realtime_test_results[9]\n",
    "\n",
    "# WPMS\n",
    "wpms = realtime_test_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.functional import edit_distance\n",
    "# Showing the wer calculation is legit!\n",
    "assert edit_distance(decodes[0].split(), gts[0].split())/len(gts[0].split()) == wers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pseudoblocking\n",
    "def parceled_metric(vals, lengths=None, metric=np.mean, parcelation=10):\n",
    "    # This function will do the pseudoblocking!\n",
    "    dist = []\n",
    "    cur_list = []\n",
    "    cur_lengths = []\n",
    "    if lengths is None: \n",
    "        lengths = np.ones((len(vals)))\n",
    "    for k, x in enumerate(vals):\n",
    "        cur_list.append(x)\n",
    "        if pd.isna(x): \n",
    "            cur_lengths.append(np.nan)\n",
    "        else: \n",
    "            cur_lengths.append(lengths[k])\n",
    "        if (k+1)%parcelation == 0: \n",
    "            num = np.nansum(np.array(cur_list)*np.array(cur_lengths))\n",
    "            dist.append(num/np.nansum(np.array(cur_lengths)))\n",
    "            cur_list = []\n",
    "            cur_lengths = []\n",
    "    if len(cur_list) > parcelation/2: \n",
    "        dist.append(np.nansum(np.array(cur_list)*np.array(cur_lengths))/np.nansum(np.array(cur_lengths)))\n",
    "    return np.array(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudoblocked_results = parceled_metric(wers, lengths=np.array([len(txt.split()) for txt in gts]), metric=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median WER across pseudoblocks 0.255\n"
     ]
    }
   ],
   "source": [
    "print(f'Median WER across pseudoblocks {np.median(pseudoblocked_results):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7fe8e61ebbe0>], [Text(0, 0, 'Decoded text')])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(pseudoblocked_results)\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks([0.25, 0.5, 0.75, 1.0])\n",
    "plt.title('Word Error rate')\n",
    "sns.despine()\n",
    "plt.xticks([0], ['Decoded text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This run's median WPM 76.31\n"
     ]
    }
   ],
   "source": [
    "pseudoblocked_results = parceled_metric(wpms, lengths=None, metric=np.mean)\n",
    "print(\"This run's median WPM %.2f\" %np.median(pseudoblocked_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(pseudoblocked_results)\n",
    "plt.title('Words Per Minute')\n",
    "plt.ylim(0, 85)\n",
    "plt.yticks([0, 25, 50, 75])\n",
    "sns.despine()\n",
    "plt.xticks([0], ['Decoded text'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taud",
   "language": "python",
   "name": "taud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
