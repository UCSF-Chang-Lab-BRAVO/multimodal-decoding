{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 22 23:33:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN V      Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 28%   31C    P8    24W / 250W |   3076MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN V      Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 28%   32C    P8    24W / 250W |   3143MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA TITAN V      Off  | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 36%   52C    P2    47W / 250W |   2055MiB / 12066MiB |     33%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3691569      C   ...ur_working_env/bin/python     1685MiB |\n",
      "|    0   N/A  N/A   3700637      C   ...ur_working_env/bin/python     1387MiB |\n",
      "|    1   N/A  N/A    390057      C   ...nda3/envs/ecog/bin/python     3139MiB |\n",
      "|    2   N/A  N/A    459859      C   ...silva/py3.9env/bin/python     2051MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/smetzger/torchaud/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version 1.12.0\n",
      "torch audio version 0.12.0 >= 0.12.0 needed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num_50'], dest='num_50', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, required=False, help='only used for 500 phrases', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from os.path import join\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchaudio.models import decoder\n",
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "print('torch version', torch.__version__)\n",
    "print('torch audio version', torchaudio.__version__, '>= 0.12.0 needed')\n",
    "curdir = './'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "import wandb\n",
    "\n",
    "# Change to your data dir. \n",
    "data_dir = './data/'\n",
    "device = 'cuda' # Set to cpu if you dont have a gpu avail. \n",
    "\n",
    "# Set up the experiment, can change the hyperparameters as you see fit or edit for your own models\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--decimation', \n",
    "                   default=6, \n",
    "                   type=int, \n",
    "                   help='How much to downsample neural data')\n",
    "parser.add_argument('--hidden_dim',\n",
    "                    type=int,\n",
    "                   default=512,\n",
    "                   help=\"how many hid.units in model\")\n",
    "parser.add_argument('--lr', \n",
    "                    type=float,\n",
    "                   default=1e-3,\n",
    "                   help='learning rate')\n",
    "parser.add_argument('--ks', \n",
    "                    type=int,\n",
    "                   default=2,\n",
    "                   help='ks of input conv')\n",
    "parser.add_argument('--num_layers',\n",
    "                   type=int, \n",
    "                   default=3,\n",
    "                   help='number of layers')\n",
    "parser.add_argument('--dropout', \n",
    "                   type=float, \n",
    "                   default=0.6, \n",
    "                   help='dropout amount')\n",
    "parser.add_argument('--feat_stream', \n",
    "                   type=str, \n",
    "                   default='both',\n",
    "                   help='which stream. both, hga, or raw')\n",
    "parser.add_argument('--bs',\n",
    "                   type=int, \n",
    "                   default=64, \n",
    "                   help='batch size')\n",
    "parser.add_argument('--smooth',\n",
    "                   type=int,\n",
    "                   default=0, \n",
    "                   help='how much smoothing to apply.')\n",
    "parser.add_argument('--no_normalize', \n",
    "                   action='store_false',\n",
    "                   help='Normalize the neural data or not')\n",
    "parser.add_argument('--LM_WEIGHT', \n",
    "                   help='how much the LM is weighted during beam search', \n",
    "                   type=float, \n",
    "                   default=3.23)\n",
    "parser.add_argument('--WORD_SCORE', \n",
    "                   help='word insertion score for beam',\n",
    "                    type=float,\n",
    "                    default=-.26\n",
    "                   )\n",
    "parser.add_argument('--beam_width', \n",
    "                   help='beam size to use',\n",
    "                   type=int,\n",
    "                   default=100)\n",
    "parser.add_argument('--checkpoint_dir',\n",
    "                   help='where 2 save model',\n",
    "                   type=str,\n",
    "                   default=None)\n",
    "parser.add_argument('--feedforward', \n",
    "                   help='no bidirectional',\n",
    "                   action='store_true')\n",
    "parser.add_argument('--pretrained',\n",
    "                   help='path to a pretrained model to load',\n",
    "                   type=str,\n",
    "                   default=None)\n",
    "parser.add_argument('--train_amt',\n",
    "                   help='amt of train data to use',\n",
    "                   type=float, \n",
    "                    default=1.0)\n",
    "parser.add_argument('--samples_to_trim',\n",
    "                   help='num samps back to go (to shorten window)',\n",
    "                   default=0, \n",
    "                   type=int)\n",
    "parser.add_argument('--ndense',\n",
    "                   help='Use a different number of classes for a transfer model (useful for 50 phrase transfer.)',\n",
    "                   default=40,\n",
    "                   type=int)\n",
    "parser.add_argument('--transfer_audio', \n",
    "                   help='true if transfer audio. then switch conv', \n",
    "                   action='store_true')\n",
    "parser.add_argument('--num_50', \n",
    "                   help='only used for 500 phrases',\n",
    "                   type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_str = exp_str = '--hidden_dim 512 --ks 2 --dropout 0.6 --num_layers 3 --num_50 500 --samples_to_trim 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we parse the arguments... take out the ' --train_amt 1.0 when you're actually running the script in python. \n",
    "args = vars(parser.parse_args(exp_str.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize wandb project for logging.\n",
    "wandb.init(project='b3_50_phrase_pub', \n",
    "          config=args)\n",
    "\n",
    "#### Experiment is set up, now lets load in the neural data and corresponding labels.\n",
    "##### Labels has several columns\n",
    "##### ph_label - list, each item is the phonemes from that utterance, in order <br> \n",
    "##### txt_label - the ground truth text label for that utterance <br>\n",
    "##### length - the length of the utterance, in samples, at 200 Hz <br> Needs to be divided by 6 to work with data (thats done later in this nb) \n",
    "\n",
    "##### Please ignore the rest of the columns, they were from a previous nb. \n",
    "# Each row in this df (e.g row 0) corresponds to trial 0 of the neural data\n",
    "\n",
    "labels = pd.read_hdf(join(data_dir, 'Y_50_phrase_pub_tr.h5'))\n",
    "# This test data uses the realtime blocks we used in the publication. \n",
    "labels_te = pd.read_hdf(join(data_dir, 'Y_50_phrase_pub_te.h5'))\n",
    "\n",
    "te_len = len(labels_te)\n",
    "labels = pd.concat((labels, labels_te), ignore_index=True)\n",
    "labels.head()\n",
    "\n",
    "all_labs = set(labels['txt_label'].values)\n",
    "\n",
    "all_words = []\n",
    "for a in all_labs: \n",
    "    all_words.extend(a.split(' '))\n",
    "print('vocab size', len(set(all_words)), 'words')\n",
    "\n",
    "### NEURAL DATA PREPROCESSING. This can be a very big deal for accuracy, so I suggest tinkering with parameters.\n",
    "\n",
    "# from scipy.signal import decimate\n",
    "\n",
    "def normalize(x, axis=-1, order=2):\n",
    "    \"\"\"Normalizes a Numpy array.\n",
    "    Args:\n",
    "        x: Numpy array to normalize.\n",
    "        axis: axis along which to normalize.\n",
    "        \n",
    "        order: Normalization order (e.g. `order=2` for L2 norm).\n",
    "    Returns:\n",
    "        A normalized copy of the array.\n",
    "    \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(x, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return x / np.expand_dims(l2, axis)\n",
    "\n",
    "\n",
    "import os\n",
    "if True:\n",
    "    X = np.load(os.path.join(data_dir, 'X_50_phrase_pub_tr.npy'))\n",
    "    X_te = np.load(os.path.join(data_dir, 'X_50_phrase_pub_te.npy'))\n",
    "    X = np.concatenate((X, X_te), axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600\n",
      "5650\n",
      "5700\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(X)-te_len, len(X), 50):\n",
    "    # Ensure no test in train.\n",
    "    dists = (np.sum(np.sum(np.abs(X[k]-X[:-te_len]), axis=1), axis=1))\n",
    "    assert np.sum(dists==0) ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len (X) == len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note - some models will get really good performance, others not, this training is a bit sensitive to random init\n",
    "After about 30 epochs expect to see a WER around 10% or lower. \n",
    "This is the WER we saw prior to testing the model [we only tested for the paper once after training with various random inits and choosing the best perofmring model]. This will correspond to a 3-4% WER on the test set - the test set appears to have had very strong performance, potentially becuase of the use of the avatar during this trial. Additional investigation is needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing data\n",
      "final X shape (5750, 217, 506)\n",
      "trimmed X (5750, 217, 506)\n",
      "example lexicon items\n",
      "can K AE N |\n",
      "i AY |\n",
      "get G EH T |\n",
      "my M AY |\n",
      "medicine M EH D AH S AH N |\n",
      "vocabulary size: 119\n",
      "token lm False\n",
      "(5750, 217, 506) (5750, 37)\n",
      "num samples 5400\n",
      "5400 5400\n",
      "net wer 1.0\n",
      "net cer 1.0\n",
      "greedy per 0.7703390893464609\n",
      "beam per 0.795643583454428\n",
      "gt do not make me laugh\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt is there anything i can do\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt well it sure looks like it\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt it sounds good to me\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt how are things going for you\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt help me with the chair\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt i might check that out tomorrow\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt was there something else\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt hand that to me please\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt wait for the rest of them\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt just a minute let me think about that\n",
      "t  wer: 1.00 cer: 1.00\n",
      "epoch 0 tr loss: 0.052 te_loss: 0.053\n",
      "epoch 1 tr loss: 0.039 te_loss: 0.048\n",
      "epoch 2 tr loss: 0.038 te_loss: 0.045\n",
      "net wer 0.9541825396825402\n",
      "net cer 0.9218591030423745\n",
      "greedy per 0.6916085445995459\n",
      "beam per 0.7588403610236424\n",
      "gt will i see you later\n",
      "t  wer: 1.00 cer: 1.00\n",
      "gt you are right about that\n",
      "t it wer: 1.00 cer: 0.92\n",
      "gt what have you been doing\n",
      "t do you wer: 0.80 cer: 0.83\n",
      "gt will i see you later\n",
      "t it wer: 1.00 cer: 0.90\n",
      "gt i am glad you are here\n",
      "t it wer: 1.00 cer: 0.95\n",
      "gt can i get my medicine\n",
      "t do you wer: 1.00 cer: 0.90\n",
      "gt thanks a lot it really helps\n",
      "t it wer: 0.83 cer: 0.93\n",
      "gt was there something else\n",
      "t do you wer: 1.00 cer: 0.92\n",
      "gt will i see you later\n",
      "t it wer: 1.00 cer: 0.90\n",
      "gt it happens to be my favorite\n",
      "t do you wer: 1.00 cer: 0.86\n",
      "gt thanks for stopping by\n",
      "t it is wer: 1.00 cer: 0.91\n",
      "epoch 3 tr loss: 0.035 te_loss: 0.047\n",
      "epoch 4 tr loss: 0.033 te_loss: 0.040\n",
      "epoch 5 tr loss: 0.031 te_loss: 0.041\n",
      "net wer 0.9150992063492069\n",
      "net cer 0.8502509722411503\n",
      "greedy per 0.6017368060087126\n",
      "beam per 0.7163099004938787\n",
      "gt believe me it is better\n",
      "t it is it wer: 0.60 cer: 0.70\n",
      "gt well it sure looks like it\n",
      "t what are you wer: 1.00 cer: 0.73\n",
      "gt i might check that out tomorrow\n",
      "t that wer: 0.83 cer: 0.87\n",
      "gt how long do you think it will be\n",
      "t it is wer: 0.88 cer: 0.88\n",
      "gt i might check that out tomorrow\n",
      "t are you wer: 1.00 cer: 0.87\n",
      "gt well it sure looks like it\n",
      "t it is wer: 0.83 cer: 0.85\n",
      "gt i thought it would be good for me\n",
      "t it is wer: 0.88 cer: 0.88\n",
      "gt hi how are things going\n",
      "t it is it wer: 1.00 cer: 0.74\n",
      "gt thanks a lot it really helps\n",
      "t it is it wer: 0.83 cer: 0.86\n",
      "gt thanks a lot it really helps\n",
      "t it is wer: 0.83 cer: 0.86\n",
      "gt i think you are wonderful\n",
      "t it is wer: 1.00 cer: 0.88\n",
      "epoch 6 tr loss: 0.030 te_loss: 0.038\n",
      "epoch 7 tr loss: 0.030 te_loss: 0.036\n",
      "epoch 8 tr loss: 0.027 te_loss: 0.032\n",
      "net wer 0.6186448412698412\n",
      "net cer 0.521891605518592\n",
      "greedy per 0.4491996075268315\n",
      "beam per 0.47207311682207065\n",
      "gt help me with the chair\n",
      "t help me there wer: 0.60 cer: 0.50\n",
      "gt it happens to be my favorite\n",
      "t it is wer: 0.83 cer: 0.86\n",
      "gt do not be afraid to ask me questions\n",
      "t it is wer: 1.00 cer: 0.86\n",
      "gt how often do you do this\n",
      "t do you this wer: 0.50 cer: 0.54\n",
      "gt i am sorry but i have an appointment now\n",
      "t do not be now wer: 0.89 cer: 0.78\n",
      "gt it sounds good to me\n",
      "t it is good to me wer: 0.20 cer: 0.25\n",
      "gt great to see you again\n",
      "t it wer: 1.00 cer: 0.95\n",
      "gt i thought it would be good for me\n",
      "t will you be me wer: 0.75 cer: 0.73\n",
      "gt help me with the chair\n",
      "t how is it wer: 1.00 cer: 0.77\n",
      "gt let me tell you what i did\n",
      "t do you that wer: 0.86 cer: 0.69\n",
      "gt what have you been doing\n",
      "t what have you been doing wer: 0.00 cer: 0.00\n",
      "epoch 9 tr loss: 0.023 te_loss: 0.027\n",
      "epoch 10 tr loss: 0.020 te_loss: 0.025\n",
      "epoch 11 tr loss: 0.017 te_loss: 0.020\n",
      "net wer 0.29400198412698414\n",
      "net cer 0.24424724936863482\n",
      "greedy per 0.28050378391013403\n",
      "beam per 0.22912316752244383\n",
      "gt what time will you be home\n",
      "t it is good to me wer: 1.00 cer: 0.65\n",
      "gt how often do you do this\n",
      "t you do this wer: 0.50 cer: 0.54\n",
      "gt you are right about that\n",
      "t you are right about that wer: 0.00 cer: 0.00\n",
      "gt how often do you do this\n",
      "t how often do you do this wer: 0.00 cer: 0.00\n",
      "gt will i see you later\n",
      "t will i see you later wer: 0.00 cer: 0.00\n",
      "gt i will keep an eye on that for you\n",
      "t i will keep an eye on that for you wer: 0.00 cer: 0.00\n",
      "gt i will keep an eye on that for you\n",
      "t i will keep an eye on that for you wer: 0.00 cer: 0.00\n",
      "gt you are right about that\n",
      "t you are right about that wer: 0.00 cer: 0.00\n",
      "gt thanks for telling me\n",
      "t thanks for telling me wer: 0.00 cer: 0.00\n",
      "gt well it sure looks like it\n",
      "t well it sure looks like it wer: 0.00 cer: 0.00\n",
      "gt great to see you again\n",
      "t what do you doing wer: 0.80 cer: 0.55\n",
      "epoch 12 tr loss: 0.014 te_loss: 0.018\n",
      "epoch 13 tr loss: 0.012 te_loss: 0.017\n",
      "epoch 14 tr loss: 0.011 te_loss: 0.016\n",
      "net wer 0.2140099206349206\n",
      "net cer 0.1778312542549393\n",
      "greedy per 0.22359486155394323\n",
      "beam per 0.1663187433998921\n",
      "gt how often do you do this\n",
      "t how often do you do this wer: 0.00 cer: 0.00\n",
      "gt i will meet you there\n",
      "t i will meet you there wer: 0.00 cer: 0.00\n",
      "gt what do you think of my artificial voice\n",
      "t what do you think of my artificial voice wer: 0.00 cer: 0.00\n",
      "gt well it sure looks like it\n",
      "t well it sure looks like it wer: 0.00 cer: 0.00\n",
      "gt what do you think of my artificial voice\n",
      "t what you think of it wer: 0.50 cer: 0.53\n",
      "gt is there anything i can do\n",
      "t will get to that wer: 1.00 cer: 0.81\n",
      "gt what have you been doing\n",
      "t what have you been doing wer: 0.00 cer: 0.00\n",
      "gt just a minute let me think about that\n",
      "t just a minute let me think about that wer: 0.00 cer: 0.00\n",
      "gt thank you it is looking good\n",
      "t it is looking good wer: 0.33 cer: 0.36\n",
      "gt i think you are wonderful\n",
      "t i think you are wonderful wer: 0.00 cer: 0.00\n",
      "gt do you really think so\n",
      "t do you think so wer: 0.20 cer: 0.32\n",
      "epoch 15 tr loss: 0.011 te_loss: 0.014\n",
      "epoch 16 tr loss: 0.009 te_loss: 0.014\n",
      "epoch 17 tr loss: 0.008 te_loss: 0.014\n",
      "net wer 0.15360317460317463\n",
      "net cer 0.12760952462392677\n",
      "greedy per 0.17672525959275234\n",
      "beam per 0.12288229466200928\n",
      "gt you are right about that\n",
      "t you are right about that wer: 0.00 cer: 0.00\n",
      "gt i might check that out tomorrow\n",
      "t i might check that out tomorrow wer: 0.00 cer: 0.00\n",
      "gt believe me it is better\n",
      "t believe me it is better wer: 0.00 cer: 0.00\n",
      "gt believe me it is better\n",
      "t believe me it is better wer: 0.00 cer: 0.00\n",
      "gt can i get my medicine\n",
      "t can i get my medicine wer: 0.00 cer: 0.00\n",
      "gt hi how are things going\n",
      "t how are things going wer: 0.20 cer: 0.13\n",
      "gt i am doing well today\n",
      "t i am doing well today wer: 0.00 cer: 0.00\n",
      "gt how are things going for you\n",
      "t how are things going for you wer: 0.00 cer: 0.00\n",
      "gt thank you it is looking good\n",
      "t thank you it is looking good wer: 0.00 cer: 0.00\n",
      "gt i am sorry but i have an appointment now\n",
      "t i am sorry but i have an appointment now wer: 0.00 cer: 0.00\n",
      "gt just a minute let me think about that\n",
      "t just a minute let me think about that wer: 0.00 cer: 0.00\n",
      "epoch 18 tr loss: 0.007 te_loss: 0.011\n",
      "epoch 19 tr loss: 0.007 te_loss: 0.013\n",
      "epoch 20 tr loss: 0.007 te_loss: 0.012\n",
      "net wer 0.16885912698412692\n",
      "net cer 0.13641748040801338\n",
      "greedy per 0.17862935741495825\n",
      "beam per 0.12818285317804096\n",
      "gt i think this is pretty good\n",
      "t i think this is pretty good wer: 0.00 cer: 0.00\n",
      "gt it happens to be my favorite\n",
      "t are be wer: 0.83 cer: 0.82\n",
      "gt it happens to be my favorite\n",
      "t it happens to be my favorite wer: 0.00 cer: 0.00\n",
      "gt thanks a lot it really helps\n",
      "t thanks a lot it really helps wer: 0.00 cer: 0.00\n",
      "gt hi how are things going\n",
      "t are things going wer: 0.40 cer: 0.30\n",
      "gt thanks a lot it really helps\n",
      "t thanks a lot it really helps wer: 0.00 cer: 0.00\n",
      "gt i am sorry but i have an appointment now\n",
      "t me it is wer: 1.00 cer: 0.85\n",
      "gt do you really think so\n",
      "t do you really think so wer: 0.00 cer: 0.00\n",
      "gt do you really think so\n",
      "t do you really think so wer: 0.00 cer: 0.00\n",
      "gt i think you are wonderful\n",
      "t i think you are wonderful wer: 0.00 cer: 0.00\n",
      "gt give me a few minutes\n",
      "t give me a few minutes wer: 0.00 cer: 0.00\n",
      "epoch 21 tr loss: 0.006 te_loss: 0.013\n",
      "epoch 22 tr loss: 0.006 te_loss: 0.011\n",
      "epoch 23 tr loss: 0.005 te_loss: 0.011\n",
      "net wer 0.13803769841269842\n",
      "net cer 0.10934236418969061\n",
      "greedy per 0.15471649536911486\n",
      "beam per 0.1099928394625667\n",
      "gt it happens to be my favorite\n",
      "t thanks for it wer: 1.00 cer: 0.68\n",
      "gt i might check that out tomorrow\n",
      "t is there anything i can do wer: 1.00 cer: 0.77\n",
      "gt will you do me a favor\n",
      "t will you do me a favor wer: 0.00 cer: 0.00\n",
      "gt i will meet you there\n",
      "t i will meet you there wer: 0.00 cer: 0.00\n",
      "gt was there something else\n",
      "t was there something else wer: 0.00 cer: 0.00\n",
      "gt was there something else\n",
      "t was there something else wer: 0.00 cer: 0.00\n",
      "gt how are things going for you\n",
      "t how are things going for you wer: 0.00 cer: 0.00\n",
      "gt help me with the chair\n",
      "t help me with the chair wer: 0.00 cer: 0.00\n",
      "gt i think this is pretty good\n",
      "t i think this is pretty good wer: 0.00 cer: 0.00\n",
      "gt do not be afraid to ask me questions\n",
      "t do not be afraid ask me questions wer: 0.12 cer: 0.08\n",
      "gt well it sure looks like it\n",
      "t well it sure looks like it wer: 0.00 cer: 0.00\n",
      "epoch 24 tr loss: 0.005 te_loss: 0.011\n",
      "epoch 25 tr loss: 0.005 te_loss: 0.011\n",
      "epoch 26 tr loss: 0.004 te_loss: 0.010\n",
      "net wer 0.1373452380952381\n",
      "net cer 0.10740720054839974\n",
      "greedy per 0.14566233639493256\n",
      "beam per 0.10589872962379068\n",
      "gt what are you looking for\n",
      "t what are you looking for wer: 0.00 cer: 0.00\n",
      "gt i might check that out tomorrow\n",
      "t i might check that out tomorrow wer: 0.00 cer: 0.00\n",
      "gt is there anything i can do\n",
      "t do you think so wer: 1.00 cer: 0.69\n",
      "gt tell me about yourself\n",
      "t tell me about yourself wer: 0.00 cer: 0.00\n",
      "gt thanks a lot it really helps\n",
      "t thanks a lot it really helps wer: 0.00 cer: 0.00\n",
      "gt i will meet you there\n",
      "t i will meet you there wer: 0.00 cer: 0.00\n",
      "gt i am doing well today\n",
      "t i am doing well today wer: 0.00 cer: 0.00\n",
      "gt i think you are wonderful\n",
      "t i think you are wonderful wer: 0.00 cer: 0.00\n",
      "gt will i see you later\n",
      "t will i see you later wer: 0.00 cer: 0.00\n",
      "gt help me with the chair\n",
      "t help me with the chair wer: 0.00 cer: 0.00\n",
      "gt how long do you think it will be\n",
      "t how long do you think it will be wer: 0.00 cer: 0.00\n",
      "epoch 27 tr loss: 0.004 te_loss: 0.010\n",
      "epoch 28 tr loss: 0.004 te_loss: 0.010\n",
      "epoch 29 tr loss: 0.004 te_loss: 0.011\n",
      "net wer 0.12357738095238098\n",
      "net cer 0.09734293740679607\n",
      "greedy per 0.13606121474458402\n",
      "beam per 0.09708101461475066\n",
      "gt i am glad you are here\n",
      "t i am glad you are here wer: 0.00 cer: 0.00\n",
      "gt do you really think so\n",
      "t do you really think so wer: 0.00 cer: 0.00\n",
      "gt hi how are things going\n",
      "t hi how are things going wer: 0.00 cer: 0.00\n",
      "gt have i not met you before\n",
      "t it happens to favorite wer: 1.00 cer: 0.80\n",
      "gt how long do you think it will be\n",
      "t how long do you think it will be wer: 0.00 cer: 0.00\n",
      "gt how often do you do this\n",
      "t how often do you do this wer: 0.00 cer: 0.00\n",
      "gt thanks for stopping by\n",
      "t thanks for stopping by wer: 0.00 cer: 0.00\n",
      "gt i might check that out tomorrow\n",
      "t i might check that out tomorrow wer: 0.00 cer: 0.00\n",
      "gt i will meet you there\n",
      "t i will meet you there wer: 0.00 cer: 0.00\n",
      "gt give me a few minutes\n",
      "t give me a few minutes wer: 0.00 cer: 0.00\n",
      "gt hand that to me please\n",
      "t what do you to me wer: 0.80 cer: 0.68\n",
      "epoch 30 tr loss: 0.004 te_loss: 0.011\n",
      "epoch 31 tr loss: 0.004 te_loss: 0.011\n",
      "epoch 32 tr loss: 0.004 te_loss: 0.011\n",
      "net wer 0.12568055555555552\n",
      "net cer 0.10151774273789524\n",
      "greedy per 0.13017863878044678\n",
      "beam per 0.09909418729032743\n",
      "gt i might check that out tomorrow\n",
      "t is there anything i can do wer: 1.00 cer: 0.77\n",
      "gt tell me about yourself\n",
      "t tell me about yourself wer: 0.00 cer: 0.00\n",
      "gt was there something else\n",
      "t was there something else wer: 0.00 cer: 0.00\n",
      "gt hand that to me please\n",
      "t hand that to me please wer: 0.00 cer: 0.00\n",
      "gt i am sorry but i have an appointment now\n",
      "t i am sorry but i have an appointment now wer: 0.00 cer: 0.00\n",
      "gt was there something else\n",
      "t was there something else wer: 0.00 cer: 0.00\n",
      "gt i will meet you there\n",
      "t i will meet you there wer: 0.00 cer: 0.00\n",
      "gt would you like to go with me\n",
      "t would you like to go with me wer: 0.00 cer: 0.00\n",
      "gt was there something else\n",
      "t was there something else wer: 0.00 cer: 0.00\n",
      "gt can i get my medicine\n",
      "t can i get my medicine wer: 0.00 cer: 0.00\n",
      "gt you are right about that\n",
      "t you are right about that wer: 0.00 cer: 0.00\n",
      "epoch 33 tr loss: 0.004 te_loss: 0.010\n",
      "epoch 34 tr loss: 0.003 te_loss: 0.010\n",
      "epoch 35 tr loss: 0.003 te_loss: 0.011\n",
      "net wer 0.09512499999999999\n",
      "net cer 0.07639591706602576\n",
      "greedy per 0.1235194986447818\n",
      "beam per 0.07771902310982265\n",
      "gt it is good to see you\n",
      "t it is good to see you wer: 0.00 cer: 0.00\n",
      "gt i will keep an eye on that for you\n",
      "t i will keep an eye on that for you wer: 0.00 cer: 0.00\n",
      "gt give me a few minutes\n",
      "t give me a few minutes wer: 0.00 cer: 0.00\n",
      "gt thanks a lot it really helps\n",
      "t thanks a lot it really helps wer: 0.00 cer: 0.00\n",
      "gt well it sure looks like it\n",
      "t well it sure looks like it wer: 0.00 cer: 0.00\n",
      "gt what do you think about that\n",
      "t what are you looking for wer: 0.67 cer: 0.61\n",
      "gt thanks for telling me\n",
      "t thanks for telling me wer: 0.00 cer: 0.00\n",
      "gt tell me about yourself\n",
      "t tell me about yourself wer: 0.00 cer: 0.00\n",
      "gt i will talk to you soon\n",
      "t i will talk to you soon wer: 0.00 cer: 0.00\n",
      "gt thanks a lot it really helps\n",
      "t thanks a lot it really helps wer: 0.00 cer: 0.00\n",
      "gt i am sorry but i have an appointment now\n",
      "t i am sorry but i have an appointment now wer: 0.00 cer: 0.00\n",
      "epoch 36 tr loss: 0.003 te_loss: 0.009\n",
      "epoch 37 tr loss: 0.003 te_loss: 0.009\n",
      "epoch 38 tr loss: 0.003 te_loss: 0.011\n",
      "net wer 0.10175000000000001\n",
      "net cer 0.08099023055626314\n",
      "greedy per 0.12195885921049138\n",
      "beam per 0.08186929435672537\n",
      "gt it happens to be my favorite\n",
      "t it happens to be my favorite wer: 0.00 cer: 0.00\n",
      "gt well it sure looks like it\n",
      "t well it sure looks like it wer: 0.00 cer: 0.00\n",
      "gt what are you looking for\n",
      "t what are you looking for wer: 0.00 cer: 0.00\n",
      "gt i think you are wonderful\n",
      "t i think this is pretty good wer: 0.80 cer: 0.72\n",
      "gt thanks a lot it really helps\n",
      "t thanks a lot it really helps wer: 0.00 cer: 0.00\n",
      "gt was there something else\n",
      "t was there something else wer: 0.00 cer: 0.00\n",
      "gt will i see you later\n",
      "t will i see you later wer: 0.00 cer: 0.00\n",
      "gt i am sorry but i have an appointment now\n",
      "t i am sorry but i have an appointment now wer: 0.00 cer: 0.00\n",
      "gt will i see you later\n",
      "t will i see you later wer: 0.00 cer: 0.00\n",
      "gt how is the weather today\n",
      "t how is the weather today wer: 0.00 cer: 0.00\n",
      "gt what time will you be home\n",
      "t i thought it would be for me wer: 1.00 cer: 0.69\n",
      "epoch 39 tr loss: 0.003 te_loss: 0.011\n",
      "epoch 40 tr loss: 0.003 te_loss: 0.014\n",
      "epoch 41 tr loss: 0.003 te_loss: 0.009\n",
      "net wer 0.11994444444444445\n",
      "net cer 0.0904178881806599\n",
      "greedy per 0.11404898258975185\n",
      "beam per 0.08984748689525578\n",
      "gt do you really think so\n",
      "t do you really think so wer: 0.00 cer: 0.00\n",
      "gt i thought it would be good for me\n",
      "t i thought it would be good for me wer: 0.00 cer: 0.00\n",
      "gt do not be afraid to ask me questions\n",
      "t do not be afraid to ask me questions wer: 0.00 cer: 0.00\n",
      "gt help me with the chair\n",
      "t help me with the chair wer: 0.00 cer: 0.00\n",
      "gt would you like to go with me\n",
      "t would you like to go with me wer: 0.00 cer: 0.00\n",
      "gt well it sure looks like it\n",
      "t well it sure looks like it wer: 0.00 cer: 0.00\n",
      "gt will i see you later\n",
      "t will i see you later wer: 0.00 cer: 0.00\n",
      "gt great to see you again\n",
      "t great to see you again wer: 0.00 cer: 0.00\n",
      "gt was there something else\n",
      "t was there something else wer: 0.00 cer: 0.00\n",
      "gt thank you it is looking good\n",
      "t thank you it is looking good wer: 0.00 cer: 0.00\n",
      "gt help me with the chair\n",
      "t help me with the chair wer: 0.00 cer: 0.00\n",
      "epoch 42 tr loss: 0.003 te_loss: 0.010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 137>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    202\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 203\u001b[0m model \u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_search_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Currently we only use one fold for model dev. \u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/userdata/smetzger/repos/pub_code/text/train/ctc_trainer.py:164\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_loader, test_loader, optimizer, device, texts, greedy, beam_search_decoder, tokens, patience, start_eval, wandb_log, max_epochs, wercalcrate, checkpoint_dir, train, printall, print_hypo, text_direct)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m--> 164\u001b[0m         tr_loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    166\u001b[0m         tr_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[0;32m/userdata/smetzger/repos/pub_code/text/train/ctc_trainer.py:34\u001b[0m, in \u001b[0;36mtrain_f\u001b[0;34m(model, train_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     33\u001b[0m total_samps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(),\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/userdata/smetzger/torchaud/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userdata/smetzger/torchaud/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "\n",
    "if args['no_normalize']:\n",
    "    print('normalizing data')\n",
    "    X[:, :, :X.shape[-1]//2] = normalize(X[:, :, :X.shape[-1]//2])\n",
    "    X[:, :, X.shape[-1]//2:] = normalize(X[:, :, X.shape[-1]//2:])\n",
    "else: \n",
    "    print('no normalization.')\n",
    "    \n",
    "# select feature stream\n",
    "if args['feat_stream'] == 'hga':\n",
    "    X = X[:, :, :X.shape[-1]//2]\n",
    "elif args['feat_stream'] == 'raw':\n",
    "    X = X[:, :, X.shape[-1]//2:]\n",
    "print('final X shape', X.shape)\n",
    "\n",
    "if args['samples_to_trim']>0:\n",
    "    X = X[:, :-args['samples_to_trim'], :]\n",
    "print('trimmed X', X.shape)\n",
    "\n",
    "# The following code just preprocesses the phoneme labels. Can be changed to letters or sentence pieces \n",
    "# pretty easily.\n",
    "from data_loading_utilities.clean_labels import clean_labels\n",
    "labels, all_ph = clean_labels(labels)\n",
    "phone_enc  = {v:k for k,v in enumerate(sorted([a for a in list(set(all_ph)) if not a == '|']))}\n",
    "\n",
    "#### Setup the files needed for the torchaudio CTC decoder. You can see more on this here, including how to adapt it to letters. \n",
    "# https://pytorch.org/audio/main/tutorials/asr_inference_with_ctc_decoder_tutorial.html\n",
    "\n",
    "files  = download_pretrained_files(\"librispeech-4-gram\") # Download a 4-gram librispeech LM, may take a sec.\n",
    "\n",
    "# Here we're building a lexicon. Basically we just want to say how each word is pronounced\n",
    "lex = {}\n",
    "for k,v in zip(labels['txt_label'], labels['ph_label']):\n",
    "    if not '|' in v:\n",
    "        lex[k] = ' '.join(v) + ' |'\n",
    "    else: \n",
    "        v  = '_'.join(v)\n",
    "        v = v.split('|')\n",
    "        for kk, vv in zip(k.split(' '), v):\n",
    "            vv = vv.replace('_', ' ').strip() + ' |'\n",
    "            if not kk == '':\n",
    "                lex[kk] = vv\n",
    "strings = []\n",
    "for k, v in lex.items():\n",
    "    string = k + ' ' + v\n",
    "    strings.append(string)\n",
    "    \n",
    "strings =  [s for s in strings if len(s) > 3]\n",
    "f = open(join(curdir, \"for_ctc/lexicon_phrases_50.txt\"), \"w\")\n",
    "f.writelines([s+ '\\n' for s in strings])\n",
    "f.close()\n",
    "\n",
    "print('example lexicon items')\n",
    "for s in strings[:5]:\n",
    "    print(s)\n",
    "print('vocabulary size:', len(strings))\n",
    "\n",
    "tokens = ['-', '|'] + list(phone_enc.keys())\n",
    "with open(join(curdir, 'for_ctc/tokens_phrases_50.txt'), 'w') as f:\n",
    "    f.writelines([t + '\\n' for t in tokens])\n",
    "\n",
    "# Final encoder for labels to go from tokens to labels.\n",
    "enc_final = {v:k for k,v in enumerate(tokens)}\n",
    "\n",
    "# Initialize beam search \n",
    "\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torchaudio.functional import edit_distance\n",
    "# Set up a beam search decoder. Will take a second the first time. \n",
    "beam_search_decoder= ctc_decoder(\n",
    "    lexicon = join(curdir, 'for_ctc/lexicon_phrases_50.txt'),\n",
    "    tokens = join(curdir, 'for_ctc/tokens_phrases_50.txt'),\n",
    "    lm =join(curdir, 'custom_lms/50_phrase_lm.binary'),\n",
    "    nbest=3,\n",
    "    beam_size=100, #args['beam_width'],\n",
    "    lm_weight=args['LM_WEIGHT'],\n",
    "    word_score=args['WORD_SCORE'],\n",
    "    sil_token = '|', \n",
    "    unk_word = '<unk>',\n",
    ")\n",
    "\n",
    "# Get a greedy decoder ready as well,can be useful to see how much LM is helping.\n",
    "from train.ctc_decoding import GreedyCTCDecoder\n",
    "greedy_decoder = GreedyCTCDecoder(tokens)\n",
    "greedy = GreedyCTCDecoder(labels=list(enc_final.keys()))\n",
    "\n",
    "# Get neural data/targets ready for training\n",
    "\n",
    "# Prepare neural and target data for CTC loss\n",
    "\n",
    "y_final = []\n",
    "for t, targ in zip(labels['txt_label'], labels['ph_label']):\n",
    "    cur_y = []\n",
    "    cur_y.append(enc_final['|'])\n",
    "    for ph in targ:\n",
    "        cur_y.append(enc_final[ph])\n",
    "    cur_y.append(enc_final['|'])\n",
    "    y_final.append(cur_y)\n",
    "\n",
    "y_final_ = -1*np.ones((len(y_final), np.max([len(y) for y in y_final])))\n",
    "targ_lengths =[]\n",
    "for k, y in enumerate(y_final):\n",
    "    y_final_[k, :len(y)] = np.array(y)\n",
    "    targ_lengths.append(len(y))\n",
    "targ_lengths = np.array(targ_lengths)\n",
    "Y = y_final_\n",
    "\n",
    "lens = [(l//args['decimation']) for l in labels['length']] # Adjust lengths based on decimation. \n",
    "# Finalize the lengths. \n",
    "outlens = targ_lengths\n",
    "lens = np.array(lens)\n",
    "lens = lens - args['samples_to_trim']\n",
    "lens = [min(l, X.shape[1]) for l in lens]\n",
    "lens = np.array(lens)# Some lengths may be a sample over. \n",
    "gt_text = labels['txt_label'].values\n",
    "\n",
    "# Set up cv folds. We train on 95% of the data, test on heldout 5%\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "trainsets = []\n",
    "inds = np.arange(len(X))\n",
    "np.random.seed(1337)\n",
    "np.random.shuffle(inds)\n",
    "for k in range(10): \n",
    "    te_inds = sorted(inds)[-te_len:] #[k*(len(inds)//20): (k+1)*(len(inds)//20)]\n",
    "    tr_inds = [i for i in inds if not i in te_inds] \n",
    "    val_inds = tr_inds[-200:]\n",
    "    tr_inds = [t for t in tr_inds if not t in val_inds]\n",
    "    trainsets.append((tr_inds, val_inds, te_inds))\n",
    "\n",
    "### Train the neural network. Every 3 trials the wer/cer are evaluated. \n",
    "\n",
    "from train.ctc_trainer import train_loop\n",
    "from models.cnn_rnn import FlexibleCnnRnnClassifier\n",
    "\n",
    "for train, val, test in trainsets:\n",
    "    # Train test split, plus load into dataset\n",
    "        # Train test split, plus load into dataset\n",
    "    train_amt = int(len(train)*args['train_amt'])\n",
    "    print('num samples', train_amt)\n",
    "    wandb.log({'num_samples':train_amt})\n",
    "    train = train[:train_amt]\n",
    "    print(len(train), train_amt)\n",
    "    \n",
    "    X_tr, X_te, X_v = X[train], X[test], X[val]\n",
    "    Y_tr, Y_te , Y_v = Y[train], Y[test], Y[val]\n",
    "    lens_tr, lens_te, lens_v = lens[train], lens[test], lens[val]\n",
    "    inds_tr, inds_te, inds_v = np.array(train), np.array(test), np.array(val) # for loading text labels.\n",
    "    outlens_tr, outlens_te, outlens_v = outlens[train], outlens[test], outlens[val]\n",
    "    \n",
    "    # Make datasets\n",
    "    train_dset = TensorDataset(torch.from_numpy(X_tr.copy()), \n",
    "                               torch.from_numpy(Y_tr.copy()), \n",
    "                              torch.from_numpy(lens_tr.copy()), \n",
    "                              torch.from_numpy(outlens_tr.copy()), \n",
    "                              torch.from_numpy(inds_tr.copy()))\n",
    "    test_dset = TensorDataset(torch.from_numpy(X_te.copy()), \n",
    "                              torch.from_numpy(Y_te.copy()), \n",
    "                             torch.from_numpy(lens_te.copy()),\n",
    "                             torch.from_numpy(outlens_te.copy()), \n",
    "                             torch.from_numpy(inds_te.copy()))\n",
    "    val_dset = TensorDataset(torch.from_numpy(X_v.copy()), \n",
    "                              torch.from_numpy(Y_v.copy()), \n",
    "                             torch.from_numpy(lens_v.copy()),\n",
    "                             torch.from_numpy(outlens_v.copy()), \n",
    "                             torch.from_numpy(inds_v.copy()))\n",
    "    \n",
    "    # TODO: Add transforms from torchaudio.transforms\n",
    "    train_loader = DataLoader(train_dset, batch_size=args['bs'], shuffle=True) \n",
    "    val_loader = DataLoader(val_dset, batch_size=args['bs'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dset, batch_size=args['bs'], shuffle=False)\n",
    "    \n",
    "    # Initialize the model. \n",
    "    if not args['feedforward']:\n",
    "        if not args['pretrained'] is None: \n",
    "            n_targ = args['ndense']\n",
    "        else: \n",
    "            n_targ=len((enc_final))\n",
    "            \n",
    "        model = FlexibleCnnRnnClassifier(rnn_dim=args['hidden_dim'], KS=args['ks'], \n",
    "                                         num_layers=args['num_layers'],\n",
    "                                         dropout=args['dropout'], n_targ=n_targ,\n",
    "                                  bidirectional=True, in_channels=X_tr.shape[-1])\n",
    "    else: \n",
    "        model = FlexibleCnnRnnClassifier(rnn_dim=args['hidden_dim'], KS=args['ks'], \n",
    "                                 num_layers=args['num_layers'],\n",
    "                                 dropout=args['dropout'], n_targ=len((enc_final)),\n",
    "                          bidirectional=False, in_channels=X_tr.shape[-1])\n",
    "        \n",
    "    if not args['pretrained'] is None: \n",
    "        model.load_state_dict(torch.load(join(curdir, args['pretrained'])))\n",
    "        model.dense = nn.Linear(2*args['hidden_dim'], len((enc_final)))\n",
    "        \n",
    "        if args['transfer_audio']:\n",
    "            model.preprocessing_conv = torch.nn.Conv1d(in_channels=X_te.shape[-1],\n",
    "                                               out_channels=args['hidden_dim'],\n",
    "                                               kernel_size=args['ks'],\n",
    "                                               stride=args['ks'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    model = model.to(device)\n",
    "    model =train_loop(model, train_loader,\n",
    "                    val_loader, \n",
    "                      optimizer,\n",
    "                    device, gt_text, greedy, beam_search_decoder, tokens, start_eval=0, \n",
    "                     wandb_log=True, checkpoint_dir=args['checkpoint_dir'])\n",
    "    \n",
    "    # Currently we only use one fold for model dev. \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net wer 0.03133333333333333\n",
      "net cer 0.027093025283347862\n",
      "greedy per 0.06600146498863235\n",
      "beam per 0.027620318114687025\n",
      "gt i am glad you are here\n",
      "t i am glad you are here wer: 0.00 cer: 0.00\n",
      "gt can i get my medicine\n",
      "t can i get my medicine wer: 0.00 cer: 0.00\n",
      "gt it sounds good to me\n",
      "t it sounds good to me wer: 0.00 cer: 0.00\n",
      "gt i think this is pretty good\n",
      "t i think this is pretty good wer: 0.00 cer: 0.00\n",
      "gt how is the weather today\n",
      "t how is the weather today wer: 0.00 cer: 0.00\n",
      "gt i thought it would be good for me\n",
      "t i thought it would be good for me wer: 0.00 cer: 0.00\n",
      "gt thank you it is looking good\n",
      "t thank you it is looking good wer: 0.00 cer: 0.00\n",
      "gt how are things going for you\n",
      "t how are things going for you wer: 0.00 cer: 0.00\n",
      "gt i might check that out tomorrow\n",
      "t i might check that out for you wer: 0.33 cer: 0.19\n",
      "gt what are you looking for\n",
      "t what are you looking for wer: 0.00 cer: 0.00\n",
      "gt i think this is pretty good\n",
      "t i think this is pretty good wer: 0.00 cer: 0.00\n",
      "epoch 0 tr loss: nan te_loss: 0.006\n"
     ]
    }
   ],
   "source": [
    "# Run on test set. For early stopping, this model used a different strategy (some predictions were left to end), see paper for more details.  \n",
    "model =train_loop(model, train_loader,\n",
    "                test_loader, \n",
    "                  optimizer,\n",
    "                device, gt_text, greedy, beam_search_decoder, tokens, start_eval=0, max_epochs=1,\n",
    "                 wandb_log=True, checkpoint_dir=args['checkpoint_dir'], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taud",
   "language": "python",
   "name": "taud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
